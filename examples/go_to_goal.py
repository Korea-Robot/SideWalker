# validation_test_enhanced.py
"""
ì£¼ìš” ê¸°ëŠ¥ (ê°œì„ ëœ ë²„ì „)

ëª¨ë¸ ë¡œë“œ: ì €ì¥ëœ .pth íŒŒì¼ì—ì„œ í•™ìŠµëœ ì •ì±… ë¡œë“œ
ì‹œê°ì  ê²€ì¦: í™˜ê²½ ë Œë”ë§ìœ¼ë¡œ ì‹¤ì œ ì£¼í–‰ í™•ì¸
Goal Vector ì‹œê°í™”: ëª©í‘œ ë°©í–¥ì„ í™”ì‚´í‘œë¡œ í‘œì‹œ
ì§ì ‘ ëª©í‘œ ì¶”ì : Goal Vectorë¥¼ í–¥í•œ ì•¡ì…˜ ìƒì„±
ë¹„ë””ì˜¤ ì €ì¥: ì£¼í–‰ ê³¼ì •ì„ ë™ì˜ìƒìœ¼ë¡œ ì €ì¥
í†µê³„ ë¶„ì„: ì—¬ëŸ¬ ì—í”¼ì†Œë“œì˜ ì„±ê³µë¥  ë“± ë¶„ì„

ğŸ¯ ì‚¬ìš©ë²•
bashpython validation_test_enhanced.py
ğŸ“Š ì¶œë ¥ ê²°ê³¼

ì„¼ì„œ ì‹œê°í™”: RGB, Depth, Semantic ì¹´ë©”ë¼ ì´ë¯¸ì§€ + Goal Vector í‘œì‹œ
ì£¼í–‰ ë¹„ë””ì˜¤: validation_run.avi íŒŒì¼ë¡œ ì €ì¥ (Goal Vector ì˜¤ë²„ë ˆì´ í¬í•¨)
ìƒì„¸ í†µê³„: ì„±ê³µë¥ , ì¶©ëŒë¥ , í‰ê·  ë³´ìƒ ë“±

âš™ï¸ ì„¤ì • ë³€ê²½ í¬ì¸íŠ¸

model_path: ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½
num_episodes: í…ŒìŠ¤íŠ¸í•  ì—í”¼ì†Œë“œ ìˆ˜
max_steps: ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜
use_render=True: ì‹œê°í™” í™œì„±í™”
use_goal_tracking=True: Goal Vector ê¸°ë°˜ ì•¡ì…˜ ì‚¬ìš©

"""
import numpy as np
import torch
import cv2
import matplotlib.pyplot as plt
from pathlib import Path
import time
import math

from metaurban import SidewalkStaticMetaUrbanEnv
from metaurban.obs.mix_obs import ThreeSourceMixObservation
from metaurban.component.sensors.depth_camera import DepthCamera
from metaurban.component.sensors.rgb_camera import RGBCamera
from metaurban.component.sensors.semantic_camera import SemanticCamera

# ìµœìƒìœ„ ê²½ë¡œì—ì„œ import
# from scripts.autonomous_driving_ppo_modular import PPOAgent, PPOConfig, ObservationProcessor, RewardCalculator

# ìƒëŒ€ ê²½ë¡œë¡œ import
from scripts.core import PPOAgent, PPOConfig, ObservationProcessor, RewardCalculator

# ============================================================================
# í™˜ê²½ ì„¤ì • (ë™ì¼)
# ============================================================================
SENSOR_SIZE = (256, 160)
BASE_ENV_CFG = dict(
    use_render=True,  # ì‹œê°í™”ë¥¼ ìœ„í•´ Trueë¡œ ë³€ê²½
    map='XSOS', 
    manual_control=False, 
    crswalk_density=1, 
    object_density=0.1, 
    walk_on_all_regions=False,
    drivable_area_extension=55, 
    height_scale=1, 
    horizon=300,
    vehicle_config=dict(enable_reverse=True),
    show_sidewalk=True, 
    show_crosswalk=True,
    random_lane_width=True, 
    random_agent_model=True, 
    random_lane_num=True,
    relax_out_of_road_done=True, 
    max_lateral_dist=5.0,
    agent_observation=ThreeSourceMixObservation,
    image_observation=True,
    sensors={
        "rgb_camera": (RGBCamera, *SENSOR_SIZE),                
        "depth_camera": (DepthCamera, *SENSOR_SIZE),
        "semantic_camera": (SemanticCamera, *SENSOR_SIZE),
    },
    log_level=50,
)

class GoalVectorController:
    """Goal Vector ê¸°ë°˜ ì œì–´ê¸°"""
    
    def __init__(self, max_steer=0.5, max_throttle=0.8, goal_threshold=2.0):
        self.max_steer = max_steer
        self.max_throttle = max_throttle
        self.goal_threshold = goal_threshold
        
    def get_action_from_goal_vec(self, goal_vec, current_speed=0):
        """Goal Vectorë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•¡ì…˜ ìƒì„±"""
        goal_distance = np.linalg.norm(goal_vec)
        
        if goal_distance < 0.1:  # ë§¤ìš° ê°€ê¹Œìš´ ê²½ìš°
            return np.array([0.0, 0.0])  # ì •ì§€
        
        # ëª©í‘œ ê°ë„ ê³„ì‚° (ì°¨ëŸ‰ ì•ìª½ì´ xì¶•)
        goal_angle = math.atan2(goal_vec[1], goal_vec[0])
        
        # ì¡°í–¥ê° ê³„ì‚° (-1 ~ 1ë¡œ ì •ê·œí™”)
        steering = np.clip(goal_angle / math.pi, -1.0, 1.0) * self.max_steer
        
        # ìŠ¤ë¡œí‹€ ê³„ì‚° (ëª©í‘œê¹Œì§€ì˜ ê±°ë¦¬ì™€ í˜„ì¬ ì†ë„ ê³ ë ¤)
        desired_speed = min(goal_distance * 0.5, 15.0)  # ìµœëŒ€ 15m/s
        speed_diff = desired_speed - current_speed
        
        if speed_diff > 0:
            throttle = np.clip(speed_diff * 0.1, 0.1, self.max_throttle)
        else:
            throttle = np.clip(speed_diff * 0.05, -0.5, 0.0)  # ë¸Œë ˆì´í¬
        
        return np.array([steering, throttle])

class ValidationTester:
    """í•™ìŠµëœ ëª¨ë¸ ê²€ì¦ í´ë˜ìŠ¤ (Goal Vector ì‹œê°í™” í¬í•¨)"""
    
    def __init__(self, model_path: str, device='cuda', use_goal_tracking=False):
        self.device = device if torch.cuda.is_available() else 'cpu'
        self.use_goal_tracking = use_goal_tracking
        print(f"Using device: {self.device}")
        print(f"Goal tracking mode: {'ON' if use_goal_tracking else 'OFF'}")
        
        # Goal Vector ì œì–´ê¸°
        self.goal_controller = GoalVectorController()
        
        # ëª¨ë¸ ë¡œë“œ (Goal trackingì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ)
        if not use_goal_tracking:
            self.agent = PPOAgent(PPOConfig(), self.device)
            self.device = torch.device('cpu')
            
            # ëª¨ë¸ ë¡œë“œ
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
            self.agent.policy.load_state_dict(checkpoint['policy_state_dict'])
            self.agent.value.load_state_dict(checkpoint['value_state_dict'])
            self.agent.policy_optimizer.load_state_dict(checkpoint['policy_optimizer_state_dict'])
            self.agent.value_optimizer.load_state_dict(checkpoint['value_optimizer_state_dict'])
            self.agent.stats = checkpoint['stats']
            self.agent.policy.eval()
        
        # í™˜ê²½ ì´ˆê¸°í™” (ë Œë”ë§ í™œì„±í™”)
        self.env = SidewalkStaticMetaUrbanEnv(BASE_ENV_CFG)
        
        # ìœ í‹¸ë¦¬í‹°
        self.obs_processor = ObservationProcessor()
        self.reward_calculator = RewardCalculator()
        
    def draw_goal_vector_on_frame(self, frame, goal_vec, vehicle_pos=None):
        """í”„ë ˆì„ì— Goal Vector ì‹œê°í™”"""
        if frame is None:
            return frame
            
        h, w = frame.shape[:2]
        center_x, center_y = w // 2, h // 2
        
        # Goal Vector í¬ê¸° ì •ê·œí™” (í™”ë©´ í¬ê¸°ì— ë§ê²Œ)
        goal_distance = np.linalg.norm(goal_vec)
        if goal_distance > 0.1:
            # ë²¡í„° ë°©í–¥ì„ í™”ë©´ ì¢Œí‘œë¡œ ë³€í™˜
            scale = min(w, h) * 0.2  # í™”ì‚´í‘œ ê¸¸ì´
            arrow_end_x = int(center_x + goal_vec[0] * scale / goal_distance)
            arrow_end_y = int(center_y - goal_vec[1] * scale / goal_distance)  # yì¶• ë°˜ì „
            
            # í™”ì‚´í‘œ ê·¸ë¦¬ê¸°
            cv2.arrowedLine(frame, (center_x, center_y), (arrow_end_x, arrow_end_y), 
                          (0, 255, 0), 3, tipLength=0.3)
            
            # ëª©í‘œ ê±°ë¦¬ í…ìŠ¤íŠ¸
            cv2.putText(frame, f"Goal: {goal_distance:.1f}m", 
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            # ëª©í‘œ ê°ë„ í…ìŠ¤íŠ¸
            goal_angle = math.degrees(math.atan2(goal_vec[1], goal_vec[0]))
            cv2.putText(frame, f"Angle: {goal_angle:.1f}Â°", 
                       (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        # ì¤‘ì‹¬ì  í‘œì‹œ
        cv2.circle(frame, (center_x, center_y), 5, (255, 0, 0), -1)
        
        return frame
        
    def run_episode(self, max_steps=1000, save_video=False):
        """ë‹¨ì¼ ì—í”¼ì†Œë“œ ì‹¤í–‰ (Goal Vector ì‹œê°í™” í¬í•¨)"""
        # í™˜ê²½ ë¦¬ì…‹
        obs, _ = self.env.reset()
        nav = self.env.vehicle.navigation.get_navi_info()
        obs["goal_vec"] = np.array(nav[:2], dtype=np.float32)
        
        if not self.use_goal_tracking:
            state = self.obs_processor.preprocess_observation(obs)
        
        episode_reward = 0
        step = 0
        trajectory = []
        
        # ë¹„ë””ì˜¤ ì €ì¥ ì„¤ì •
        if save_video:
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
            out = cv2.VideoWriter('validation_run_with_goal.avi', fourcc, 20.0, (800, 600))
        
        print(f"Starting validation - Initial goal distance: {np.linalg.norm(obs['goal_vec']):.2f}")
        print(f"Control mode: {'Goal Vector Tracking' if self.use_goal_tracking else 'Trained Policy'}")
        
        while step < max_steps:
            # í–‰ë™ ì„ íƒ
            if self.use_goal_tracking:
                # Goal Vector ê¸°ë°˜ ì§ì ‘ ì œì–´
                current_speed = self.env.vehicle.speed if hasattr(self.env.vehicle, 'speed') else 0
                action = self.goal_controller.get_action_from_goal_vec(obs["goal_vec"], current_speed)
                action = torch.tensor(action, dtype=torch.float32)
            else:
                # í•™ìŠµëœ ì •ì±… ì‚¬ìš©
                with torch.no_grad():
                    action, _, _ = self.agent.select_action(state)
            
            # í™˜ê²½ ìŠ¤í…
            next_obs, _, done, truncated, info = self.env.step(action.squeeze().numpy())
            
            # goal_vec ì—…ë°ì´íŠ¸
            nav = self.env.vehicle.navigation.get_navi_info()
            next_obs["goal_vec"] = np.array(nav[:2], dtype=np.float32)
            
            # ë³´ìƒ ê³„ì‚°
            reward = self.reward_calculator.compute_reward(obs, action, next_obs, done, info)
            
            # ìƒíƒœ ì •ë³´ ì €ì¥
            trajectory.append({
                'step': step,
                'action': action.squeeze().numpy(),
                'reward': reward,
                'goal_distance': np.linalg.norm(next_obs["goal_vec"]),
                'goal_vec': next_obs["goal_vec"].copy(),
                'speed': info.get('speed', 0),
                'crash': info.get('crash', False),
                'out_of_road': info.get('out_of_road', False),
                'arrive_dest': info.get('arrive_dest', False)
            })
            
            # ë Œë”ë§ ë° ë¹„ë””ì˜¤ ì €ì¥ (Goal Vector ì˜¤ë²„ë ˆì´ í¬í•¨)
            if save_video:
                frame = self.env.render(mode='rgb_array')
                if frame is not None:
                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                    frame = cv2.resize(frame, (800, 600))
                    
                    # Goal Vector ì‹œê°í™” ì¶”ê°€
                    frame = self.draw_goal_vector_on_frame(frame, next_obs["goal_vec"])
                    
                    # ì œì–´ ëª¨ë“œ í‘œì‹œ
                    mode_text = "Goal Tracking" if self.use_goal_tracking else "Trained Policy"
                    cv2.putText(frame, f"Mode: {mode_text}", 
                               (10, frame.shape[0] - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                    
                    out.write(frame)
            
            episode_reward += reward
            step += 1
            
            # ìƒíƒœ ì¶œë ¥
            if step % 50 == 0:
                goal_dist = np.linalg.norm(next_obs["goal_vec"])
                speed = info.get('speed', 0)
                action_str = f"[{action.squeeze().numpy()[0]:.2f}, {action.squeeze().numpy()[1]:.2f}]"
                print(f"Step {step}: Action={action_str}, Reward={reward:.2f}, Goal_dist={goal_dist:.2f}, Speed={speed:.1f}")
            
            # ì¢…ë£Œ ì¡°ê±´
            if done or truncated:
                if info.get('arrive_dest', False):
                    print(f"SUCCESS! Arrived at destination in {step} steps!")
                elif info.get('crash', False):
                    print(f"CRASH! Episode ended at step {step}")
                elif info.get('out_of_road', False):
                    print(f"OUT OF ROAD! Episode ended at step {step}")
                break
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            obs = next_obs
            if not self.use_goal_tracking:
                state = self.obs_processor.preprocess_observation(obs)
        
        if save_video:
            out.release()
            filename = 'validation_run_with_goal.avi'
            print(f"Video saved as '{filename}'")
        
        print(f"Episode completed - Steps: {step}, Total Reward: {episode_reward:.2f}")
        return trajectory, episode_reward, step
    
    def run_multiple_episodes(self, num_episodes=5):
        """ì—¬ëŸ¬ ì—í”¼ì†Œë“œ ì‹¤í–‰ ë° í†µê³„"""
        results = []
        
        for i in range(num_episodes):
            print(f"\n=== Episode {i+1}/{num_episodes} ===")
            trajectory, reward, steps = self.run_episode()
            
            # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
            success = any(t['arrive_dest'] for t in trajectory)
            crash = any(t['crash'] for t in trajectory)
            
            results.append({
                'episode': i+1,
                'reward': reward,
                'steps': steps,
                'success': success,
                'crash': crash,
                'final_goal_distance': trajectory[-1]['goal_distance'] if trajectory else float('inf')
            })
        
        return results
    
def main():
    """ë©”ì¸ ê²€ì¦ í•¨ìˆ˜"""
    # ì„¤ì •
    model_path = "checkpoints/final_model.pth"
    use_goal_tracking = True  # True: Goal Vector ê¸°ë°˜ ì œì–´, False: í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©
    
    if not use_goal_tracking and not Path(model_path).exists():
        print(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        print("Goal tracking ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
        use_goal_tracking = True
    
    # ê²€ì¦ í…ŒìŠ¤í„° ì´ˆê¸°í™”
    tester = ValidationTester(model_path, use_goal_tracking=use_goal_tracking)
    
    
    # ë‹¨ì¼ ì—í”¼ì†Œë“œ ì‹¤í–‰ (ë¹„ë””ì˜¤ ì €ì¥)
    print(f"\nRunning single episode with video recording...")
    tester.run_episode(save_video=True)
    
    # ì—¬ëŸ¬ ì—í”¼ì†Œë“œ í†µê³„
    print(f"\nRunning multiple episodes for statistics...")
    tester.run_multiple_episodes(num_episodes=3)
    
if __name__ == "__main__":
    main()