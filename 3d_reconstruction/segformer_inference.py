#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# --- í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤ ---
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import numpy as np
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
# [ì¶”ê°€] Hugging Face ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ
from transformers import (
    SegformerForSemanticSegmentation, 
    SegformerImageProcessor,
    MaskFormerForInstanceSegmentation,
    AutoImageProcessor
)
from torchvision import transforms
import matplotlib.pyplot as plt
from PIL import Image as PILImage

# --- ğŸ“œ 1. ê¸°ë³¸ ì„¤ì • ë³€ìˆ˜ë“¤ ---

# êµ¬ë…í•  ROS 2 ì´ë¯¸ì§€ í† í”½ ì´ë¦„
REALSENSE_TOPIC = "/camera/camera/color/image_raw"
# REALSENSE_TOPIC = "/argus/ar0234_front_left/image_raw"
# ë¡œì»¬ ëª¨ë¸(surface, object) ì¶”ë¡  ì‹œ ì…ë ¥ í¬ê¸°
INFERENCE_SIZE = 512

# ğŸ§  2. ì¶”ë¡ ì— ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ì„¤ì •
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ğŸ”„ 3. ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì… ì„ íƒ (ì¤‘ìš”!)
# 0: surface (ë¡œì»¬), 1: object (ë¡œì»¬), 2: ade20k (HF), 3: coco (HF)
model_list = ['surface', 'object', 'ade20k', 'coco']
MODEL_TYPE_INDEX = 1  # <--- ğŸš€ ì—¬ê¸°ë¥¼ ë³€ê²½í•˜ì—¬ ëª¨ë¸ ì„ íƒ (ì˜ˆ: ade20k)
MODEL_TYPE = model_list[MODEL_TYPE_INDEX]

# ğŸ—‚ï¸ 4. ì„ íƒëœ ëª¨ë¸ì— ë”°ë¥¸ ì„¤ì • (ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ)
# (ade20k, cocoëŠ” ì´ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  HFì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ)

LOCAL_MODEL_PATHS = {
    'surface': "models/surface/surface_mask_best_lrup.pt",
    'object': "models/dynamic_object/best_model2.pth"
}

# ë¡œì»¬ ëª¨ë¸ìš© í´ë˜ìŠ¤ ì •ë³´
LOCAL_CLASS_INFO = {
    'surface': {
        'background': 0, 'caution_zone': 1, 'bike_lane': 2, 'alley': 3,
        'roadway': 4, 'braille_block': 5, 'sidewalk': 6
    },
    'object': {
        'background': 0, 'barricade': 1, 'bench': 2, 'bicycle': 3, 'bollard': 4,
        'bus': 5, 'car': 6, 'carrier': 7, 'cat': 8, 'chair': 9, 'dog': 10,
        'fire_hydrant': 11, 'kiosk': 12, 'motorcycle': 13, 'movable_signage': 14,
        'parking_meter': 15, 'person': 16, 'pole': 17, 'potted_plant': 18,
        'power_controller': 19, 'scooter': 20, 'stop': 21, 'stroller': 22,
        'table': 23, 'traffic_light': 24, 'traffic_light_controller': 25,
        'traffic_sign': 26, 'tree_trunk': 27, 'truck': 28, 'wheelchair': 29
    }
}


# --- ğŸ¤– 5. Segformer ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (ë¡œì»¬ ëª¨ë¸ ë¡œë”©ìš©) ---
# 'surface', 'object' ëª¨ë¸ì„ ë¡œë“œí•  ë•Œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.
class DirectSegFormer(nn.Module):
    def __init__(self, pretrained_model_name="nvidia/mit-b0", num_classes=7):
        super().__init__()
        try:
            self.original_model = SegformerForSemanticSegmentation.from_pretrained(
                pretrained_model_name,
                num_labels=num_classes,
                ignore_mismatched_sizes=True,
                trust_remote_code=True,
                torch_dtype=torch.float32,
                use_safetensors=True,
            )
        except ValueError as e:
            if "torch.load" in str(e):
                print(f"Warning: {e}")
                print("ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ ì—†ì´ ëª¨ë¸ êµ¬ì¡°ë§Œ ìƒì„±í•©ë‹ˆë‹¤...")
                from transformers import SegformerConfig
                config = SegformerConfig.from_pretrained(pretrained_model_name)
                config.num_labels = num_classes
                self.original_model = SegformerForSemanticSegmentation(config)
            else:
                raise e

    def forward(self, x):
        outputs = self.original_model(pixel_values=x)
        return outputs.logits

# --- ğŸš€ 6. ë©”ì¸ ROS 2 ë…¸ë“œ í´ë˜ìŠ¤ ---
class SegformerViewerNode(Node):
    def __init__(self):
        super().__init__('segformer_viewer_node')

        self.device = DEVICE
        self.model_type = MODEL_TYPE
        self.get_logger().info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device} ğŸ’»")
        self.get_logger().info(f"ì„ íƒëœ ëª¨ë¸ íƒ€ì…: {self.model_type}")

        # ğŸ§  [ê°œì„ ] ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ë™ì ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
        # id2label_map: {0: 'class_a', 1: 'class_b', ...} í˜•íƒœ
        self.model, self.processor, id2label_map = self.load_model_and_processor()
        
        # ë¡œë“œëœ ë§µì„ ê¸°ë°˜ìœ¼ë¡œ í´ë˜ìŠ¤ ì •ë³´ ì„¤ì •
        self.IDX_TO_CLASS = id2label_map
        self.CLASS_TO_IDX = {v: k for k, v in self.IDX_TO_CLASS.items()}
        self.NUM_LABELS = len(self.IDX_TO_CLASS)
        
        self.get_logger().info(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ. ì´ {self.NUM_LABELS}ê°œ í´ë˜ìŠ¤ ê°ì§€.")

        # ğŸ¨ í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ íŒ”ë ˆíŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (í´ë˜ìŠ¤ ê°œìˆ˜ì— ë§ì¶° ë™ì  ìƒì„±)
        self.color_palette = self.create_color_palette()
        
        # ğŸ”– ë²”ë¡€ ì´ë¯¸ì§€ë¥¼ ë¯¸ë¦¬ ìƒì„±í•´ë‘¡ë‹ˆë‹¤. (í´ë˜ìŠ¤ ê°œìˆ˜ì— ë§ì¶° ë™ì  ìƒì„±)
        self.legend_image = self.create_legend_image()

        # ğŸ”„ ROS ì´ë¯¸ì§€ì™€ OpenCV ì´ë¯¸ì§€ë¥¼ ë³€í™˜í•  CvBridge ê°ì²´ ìƒì„±
        self.bridge = CvBridge()
        
        # ğŸ“¨ ì´ë¯¸ì§€ í† í”½ êµ¬ë… ì„¤ì •
        self.subscription = self.create_subscription(
            Image,
            REALSENSE_TOPIC,
            self.image_callback,
            10)

        self.get_logger().info('Segformer ë·°ì–´ ë…¸ë“œê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ ê¸°ë‹¤ë¦½ë‹ˆë‹¤... ğŸ“¸')

    def load_model_and_processor(self):
        """
        [ê°œì„ ëœ í•¨ìˆ˜]
        MODEL_TYPEì— ë”°ë¼ ì ì ˆí•œ ëª¨ë¸, í”„ë¡œì„¸ì„œ, í´ë˜ìŠ¤ ë§µì„ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        if self.model_type in ['surface', 'object']:
            # --- 1. ë¡œì»¬ ëª¨ë¸ (surface, object) ë¡œë“œ ---
            self.get_logger().info(f"ë¡œì»¬ ëª¨ë¸ '{self.model_type}' ë¡œë”© ì¤‘...")
            class_map_idx_first = LOCAL_CLASS_INFO[self.model_type]
            num_classes = len(class_map_idx_first)
            
            # 1-1. ëª¨ë¸ ë¡œë“œ (DirectSegFormer ë˜í¼ ì‚¬ìš©)
            model = DirectSegFormer(num_classes=num_classes)
            model_path = LOCAL_MODEL_PATHS[self.model_type]
            try:
                checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
                new_state_dict = {}
                for key, value in checkpoint.items():
                    new_key = 'original_model.' + key if key.startswith('segformer.') or key.startswith('decode_head.') else key
                    new_state_dict[new_key] = value
                model.load_state_dict(new_state_dict, strict=False)
                self.get_logger().info(f"ë¡œì»¬ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ: '{model_path}'")
            except Exception as e:
                self.get_logger().error(f"ë¡œì»¬ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.get_logger().warn("ê²½ê³ : í•™ìŠµëœ ê°€ì¤‘ì¹˜ ì—†ì´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
            # 1-2. í”„ë¡œì„¸ì„œ ì„¤ì • (torchvision.transforms ì‚¬ìš©)
            processor = transforms.Compose([
                transforms.Resize((INFERENCE_SIZE, INFERENCE_SIZE)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ])
            
            # 1-3. í´ë˜ìŠ¤ ë§µ ë°˜í™˜ (id -> label í˜•íƒœ)
            id2label_map = {v: k for k, v in class_map_idx_first.items()}
            
            model.to(self.device).eval()
            return model, processor, id2label_map

        elif self.model_type == 'ade20k':
            # --- 2. [ì‹ ê·œ] ade20k (Segformer) ëª¨ë¸ ë¡œë“œ ---
            self.get_logger().info("Hugging Face 'ade20k' (Segformer) ëª¨ë¸ ë¡œë”© ì¤‘...")
            model_name = "nvidia/segformer-b0-finetuned-ade-512-512"
            
            processor = SegformerImageProcessor.from_pretrained(model_name)
            model = SegformerForSemanticSegmentation.from_pretrained(model_name)
            
            model.to(self.device).eval()
            # model.config.id2labelì— {0: 'wall', 1: 'building', ...} ì •ë³´ê°€ ë“¤ì–´ìˆìŒ
            return model, processor, model.config.id2label

        elif self.model_type == 'coco':
            # --- 3. [ì‹ ê·œ] coco (MaskFormer) ëª¨ë¸ ë¡œë“œ ---
            self.get_logger().info("Hugging Face 'coco' (MaskFormer) ëª¨ë¸ ë¡œë”© ì¤‘...")
            # model_name = "facebook/maskformer-swin-base-coco"
            model_name = "facebook/maskformer-swin-tiny-coco"

            
            processor = AutoImageProcessor.from_pretrained(model_name)
            model = MaskFormerForInstanceSegmentation.from_pretrained(model_name)
            
            model.to(self.device).eval()
            # model.config.id2labelì— {0: 'unlabeled', 1: 'person', ...} ì •ë³´ê°€ ë“¤ì–´ìˆìŒ
            return model, processor, model.config.id2label
        
        else:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” MODEL_TYPEì…ë‹ˆë‹¤: {self.model_type}")

    def create_color_palette(self):
        """í´ë˜ìŠ¤ë³„ ê³ ìœ  ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ìƒì„± (OpenCV BGR í˜•ì‹)"""
        # 'jet' ì»¬ëŸ¬ë§µì€ ìƒ‰ìƒ êµ¬ë¶„ì´ ì˜ ë©ë‹ˆë‹¤.
        cmap = plt.cm.get_cmap('jet', self.NUM_LABELS)
        palette = np.zeros((self.NUM_LABELS, 3), dtype=np.uint8)

        for i in range(self.NUM_LABELS):
            # 0ë²ˆ í´ë˜ìŠ¤(ë°°ê²½/unlabeled)ëŠ” ê²€ì€ìƒ‰ìœ¼ë¡œ ê³ ì •
            if i == 0: 
                palette[i] = [0, 0, 0]
                continue
            
            rgba = cmap(i)
            bgr = (int(rgba[2] * 255), int(rgba[1] * 255), int(rgba[0] * 255))
            palette[i] = bgr
        
        self.get_logger().info(f"ì´ {self.NUM_LABELS}ê°œì˜ í´ë˜ìŠ¤ìš© ì»¬ëŸ¬ íŒ”ë ˆíŠ¸ ìƒì„± ì™„ë£Œ.")
        return palette

    def create_legend_image(self):
        """[ë™ì ] í´ë˜ìŠ¤ ê°œìˆ˜ì— ë§ì¶° ë²”ë¡€ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        legend_width = 250  # í´ë˜ìŠ¤ ì´ë¦„ì´ ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í­ì„ ë„“í˜
        legend_height_per_class = 20 # ë†’ì´ë¥¼ ì¤„ì—¬ ë” ë§ì€ í´ë˜ìŠ¤ í‘œì‹œ
        legend_height = legend_height_per_class * self.NUM_LABELS
        
        # í´ë˜ìŠ¤ê°€ ë„ˆë¬´ ë§ìœ¼ë©´(ì˜ˆ: 150ê°œ) ìµœëŒ€ ë†’ì´ ì œí•œ
        max_height = 1080 # (FHD ë†’ì´)
        if legend_height > max_height:
            legend_height = max_height
            legend_height_per_class = legend_height / self.NUM_LABELS

        legend_img = np.full((legend_height, legend_width, 3), 255, dtype=np.uint8)

        for i in range(self.NUM_LABELS):
            # ë²”ë¡€ê°€ ì´ë¯¸ì§€ë¥¼ ì´ˆê³¼í•˜ë©´ ì¤‘ë‹¨
            y_pos = int(i * legend_height_per_class)
            if y_pos > legend_height - legend_height_per_class:
                break

            class_name = self.IDX_TO_CLASS.get(i, 'Unknown')
            color_bgr = self.color_palette[i]
            
            swatch_start = (10, y_pos + 2)
            swatch_end = (30, y_pos + int(legend_height_per_class * 0.8))
            text_pos = (35, y_pos + int(legend_height_per_class * 0.7))

            cv2.rectangle(legend_img, swatch_start, swatch_end, 
                          (int(color_bgr[0]), int(color_bgr[1]), int(color_bgr[2])), -1)
            
            cv2.putText(legend_img, f"{i}: {class_name}", text_pos, 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1, cv2.LINE_AA)
            
        return legend_img

    def preprocess_image(self, cv_image):
        """[ê°œì„ ] ëª¨ë¸ íƒ€ì…ì— ë§ëŠ” í”„ë¡œì„¸ì„œë¡œ ì´ë¯¸ì§€ë¥¼ ë³€í™˜"""
        
        # 1. OpenCV(BGR) -> PIL(RGB) ì´ë¯¸ì§€ë¡œ ë³€í™˜ (ëª¨ë“  í”„ë¡œì„¸ì„œê°€ ì„ í˜¸)
        rgb_image_np = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)
        pil_image = PILImage.fromarray(rgb_image_np)

        if self.model_type in ['surface', 'object']:
            # 2-1. ë¡œì»¬ ëª¨ë¸ (torchvision.transforms)
            input_tensor = self.processor(pil_image)
            # [C, H, W] -> [1, C, H, W] ë°°ì¹˜ ì°¨ì› ì¶”ê°€ ë° ë””ë°”ì´ìŠ¤ ì „ì†¡
            return input_tensor.unsqueeze(0).to(self.device)
        
        else:
            # 2-2. HF ëª¨ë¸ (ImageProcessor)
            # processorê°€ í…ì„œ ë³€í™˜, ì •ê·œí™”, ë°°ì¹˜ ì°¨ì› ì¶”ê°€ê¹Œì§€ ëª¨ë‘ ì²˜ë¦¬
            inputs = self.processor(images=pil_image, return_tensors="pt")
            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ì…ë ¥ì„ ë””ë°”ì´ìŠ¤ë¡œ ì „ì†¡
            return inputs.to(self.device)

    def image_callback(self, msg):
        """[í•µì‹¬] ì´ë¯¸ì§€ ìˆ˜ì‹ , ì¶”ë¡ , í›„ì²˜ë¦¬, ì‹œê°í™”"""
        try:
            img_bgr = self.bridge.imgmsg_to_cv2(msg, "bgr8")
        except Exception as e:
            self.get_logger().error(f"ROS ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return

        original_h, original_w, _ = img_bgr.shape

        # 1. ì „ì²˜ë¦¬ (ëª¨ë¸ íƒ€ì…ì— ë§ê²Œ ìë™ ìˆ˜í–‰)
        inputs = self.preprocess_image(img_bgr)

        # 2. [ì¶”ë¡  ë° í›„ì²˜ë¦¬] (ëª¨ë¸ íƒ€ì…ë³„ ë¶„ê¸°)
        # ìµœì¢… ê²°ê³¼ë¬¼: segmentation_image (HxWx3 BGR ì»¬ëŸ¬ë§µ)
        segmentation_image = np.zeros((original_h, original_w, 3), dtype=np.uint8)

        with torch.no_grad():
            if self.model_type in ['surface', 'object', 'ade20k']:
                # --- [Semantic Segmentation] ---
                
                # 'surface', 'object'ëŠ” í…ì„œ ì…ë ¥
                # 'ade20k'ëŠ” ë”•ì…”ë„ˆë¦¬ ì…ë ¥
                if self.model_type == 'ade20k':
                    outputs = self.model(**inputs)
                else: 
                    outputs = self.model(inputs) # inputs = í…ì„œ
                
                # ê³µí†µ: logits ì¶”ì¶œ
                logits = outputs.logits if hasattr(outputs, 'logits') else outputs

                # ì—…ìƒ˜í”Œë§ (ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë³µì›)
                upsampled_logits = F.interpolate(
                    logits,
                    size=(original_h, original_w),
                    mode='bilinear',
                    align_corners=False
                )
                
                # ê°€ì¥ ì ìˆ˜ê°€ ë†’ì€ í´ë˜ìŠ¤ ID(0~N)ë¥¼ í”½ì…€ë³„ë¡œ ì„ íƒ
                pred_mask_np = torch.argmax(upsampled_logits, dim=1).squeeze().cpu().numpy().astype(np.uint8)
                
                # [ì‹œê°í™”] ID ë§µ -> ì»¬ëŸ¬ BGR ì´ë¯¸ì§€ë¡œ ë³€í™˜
                segmentation_image = self.color_palette[pred_mask_np]

            elif self.model_type == 'coco':
                # --- [Panoptic Segmentation (MaskFormer)] ---
                
                # 1. ì¶”ë¡  (ë”•ì…”ë„ˆë¦¬ ì…ë ¥)
                outputs = self.model(**inputs)
                
                # 2. [ì¤‘ìš”] Panoptic í›„ì²˜ë¦¬ (í”„ë¡œì„¸ì„œ ì‚¬ìš©)
                # target_sizesë¥¼ ì›ë³¸ í¬ê¸°ë¡œ ì§€ì •í•´ì•¼ í•¨
                result = self.processor.post_process_panoptic_segmentation(
                    outputs, target_sizes=[(original_h, original_w)]
                )[0] # 0ë²ˆ = ì²« ë²ˆì§¸(ìœ ì¼í•œ) ì´ë¯¸ì§€ ê²°ê³¼
                
                # (H, W) í¬ê¸°ì˜ í…ì„œ. ê° í”½ì…€ì€ 'ì¸ìŠ¤í„´ìŠ¤ ID'ë¥¼ ê°€ì§
                pred_mask_np = result["segmentation"].cpu().numpy()
                # ì¸ìŠ¤í„´ìŠ¤ IDë³„ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: {id: 1, label_id: 15, ...})
                segment_info = result["segments_info"]

                # 3. [ì‹œê°í™”] ì¸ìŠ¤í„´ìŠ¤ ID ë§µ -> í´ë˜ìŠ¤ ì»¬ëŸ¬ ë§µìœ¼ë¡œ ë³€í™˜
                # (ë°°ê²½ì€ ì–´ì°¨í”¼ 0(ê²€ì€ìƒ‰)ì´ë¯€ë¡œ ê°ì²´ë“¤ë§Œ ìˆœíšŒí•˜ë©° ìƒ‰ì¹ )
                for info in segment_info:
                    instance_id = info['id']
                    class_id = info['label_id'] # 0=unlabeled, 1=person ...
                    
                    # íŒ”ë ˆíŠ¸ì—ì„œ í•´ë‹¹ 'í´ë˜ìŠ¤'ì˜ ìƒ‰ìƒ ì¡°íšŒ
                    color = self.color_palette[class_id % self.NUM_LABELS]
                    
                    # ID ë§µì—ì„œ ì´ ì¸ìŠ¤í„´ìŠ¤ IDì— í•´ë‹¹í•˜ëŠ” í”½ì…€ë“¤ë§Œ ê³¨ë¼ ìƒ‰ì¹ 
                    segmentation_image[pred_mask_np == instance_id] = color

        # 3. [ì‹œê°í™”] (ëª¨ë“  ëª¨ë¸ ê³µí†µ)
        # ì›ë³¸ ì´ë¯¸ì§€(60%) + ì„¸ê·¸ë©˜í…Œì´ì…˜(40%)
        overlay = cv2.addWeighted(img_bgr, 0.6, segmentation_image, 0.4, 0)
        
        # ë²”ë¡€ ì´ë¯¸ì§€ ë†’ì´ ë§ì¶”ê¸°
        h, w, _ = img_bgr.shape
        # ê°€ë¡œ-ì„¸ë¡œ ë¹„ìœ¨ ìœ ì§€í•˜ë©° ë¦¬ì‚¬ì´ì¦ˆ (FHD ë“± ì„¸ë¡œê°€ ê¸´ ë²”ë¡€ë„ ì²˜ë¦¬)
        scale_factor = h / self.legend_image.shape[0] 
        new_legend_w = int(self.legend_image.shape[1] * scale_factor)
        
        # ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´(scale_factor < 0) ì—ëŸ¬ ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìµœì†Œ 1 í”½ì…€ ë³´ì¥
        if new_legend_w <= 0: new_legend_w = 1
        if h <= 0: h = 1
            
        resized_legend = cv2.resize(self.legend_image, (new_legend_w, h), interpolation=cv2.INTER_AREA)

        # [ì›ë³¸], [ì˜¤ë²„ë ˆì´], [ë²”ë¡€] ê°€ë¡œë¡œ ì—°ê²°
        images_to_show = np.hstack((img_bgr, overlay, resized_legend))

        # í™”ë©´ì— í‘œì‹œ
        cv2.imshow("Multi-Model Segmentation Viewer", images_to_show)
        
        key = cv2.waitKey(1) & 0xFF
        if key == 27:  # 'Esc' í‚¤
            self.get_logger().info("Esc í‚¤ ì…ë ¥ ê°ì§€. ë…¸ë“œë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            rclpy.shutdown()
            cv2.destroyAllWindows()

# --- ğŸ 7. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
def main(args=None):
    rclpy.init(args=args)
    segformer_viewer_node = SegformerViewerNode()
    try:
        rclpy.spin(segformer_viewer_node)
    finally:
        segformer_viewer_node.get_logger().info("ë…¸ë“œ ì •ë¦¬ ë° ì¢…ë£Œ ì¤‘...")
        segformer_viewer_node.destroy_node()
        cv2.destroyAllWindows()
        if rclpy.ok():
            rclpy.shutdown()

if __name__ == '__main__':
    main()
