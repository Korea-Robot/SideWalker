#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import numpy as np
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import SegformerForSemanticSegmentation
from torchvision import transforms
import matplotlib.pyplot as plt
import math
from transforms3d.quaternions import quat2mat
from PIL import Image as PILImage

# ==============================================================================
# --- âš™ï¸ ì„¤ì • ë³€ìˆ˜ (Configuration) ---
# ==============================================================================
# 1. ROS í† í”½ ì´ë¦„
RGB_TOPIC = "/camera/camera/color/image_raw"
DEPTH_TOPIC = "/camera/camera/depth/image_rect_raw"

# 2. ëª¨ë¸ ë° ì¶”ë¡  ê´€ë ¨ ì„¤ì •
MODEL_PATH = "best_model2.pth"  # ì‚¬ìš©í•˜ë˜ 'best_model2.pth' ë˜ëŠ” ê°€ìž¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ëª¨ë¸
INFERENCE_SIZE = (512, 512)     # í•™ìŠµ ì‹œ ì‚¬ìš©í–ˆë˜ ì´ë¯¸ì§€ í¬ê¸° (ë„ˆë¹„, ë†’ì´)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 3. BEV(Bird's-Eye-View) ë§µ ì„¤ì •
BEV_RESOLUTION = 0.05  # BEV ë§µì˜ í•´ìƒë„ (ë¯¸í„°/í”½ì…€)
BEV_SIZE_M = 10.0      # BEV ë§µì˜ í¬ê¸° (ì •ë°©í˜•, ë¯¸í„°)
Z_MIN_ROBOT = 0.15     # ë¡œë´‡ ì¢Œí‘œê³„ ê¸°ì¤€, ìž¥ì• ë¬¼ë¡œ ì¸ì‹í•  ìµœì†Œ ë†’ì´ (ë¯¸í„°)
Z_MAX_ROBOT = 1.8      # ë¡œë´‡ ì¢Œí‘œê³„ ê¸°ì¤€, ìž¥ì• ë¬¼ë¡œ ì¸ì‹í•  ìµœëŒ€ ë†’ì´ (ë¯¸í„°)

# 4. ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„° (RealSense D435 ê¸°ì¤€, í•„ìš”ì‹œ ìˆ˜ì •)
# ì°¸ê³ : ì´ ê°’ë“¤ì€ 'ros2 topic echo /camera/camera/camera_info' ë“±ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ê²ƒì´ ê°€ìž¥ ì •í™•í•©ë‹ˆë‹¤.
CAMERA_PARAMS = {
    'rgb': {'w': 640, 'h': 480, 'fx': 615.3, 'fy': 615.3, 'cx': 320.0, 'cy': 240.0},
    'depth': {'w': 640, 'h': 480, 'fx': 386.0, 'fy': 386.0, 'cx': 321.4, 'cy': 241.2}
}

# 5. ì¹´ë©”ë¼ì˜ ë¬¼ë¦¬ì  ìœ„ì¹˜ (ë¡œë´‡ ë² ì´ìŠ¤ ì¢Œí‘œê³„ ê¸°ì¤€)
# [x, y, z] (m) / [x, y, z, w]
CAM_TRANSLATION = [0.2, 0.0, 0.5]  # ì˜ˆì‹œ: ë¡œë´‡ ì¤‘ì‹¬ì—ì„œ ì „ë°© 20cm, ë†’ì´ 50cm
CAM_QUATERNION = [0.5, -0.5, 0.5, 0.5] # ì˜ˆì‹œ: ì•„ëž˜ë¥¼ 45ë„ ë°”ë¼ë³´ëŠ” ë°©í–¥

# ==============================================================================
# --- ðŸŽ¨ í´ëž˜ìŠ¤ ì •ë³´ ë° ì»¬ëŸ¬ë§µ (Labels & Colormaps) ---
# ==============================================================================
CLASS_TO_IDX = {
    'background': 0, 'barricade': 1, 'bench': 2, 'bicycle': 3, 'bollard': 4,
    'bus': 5, 'car': 6, 'carrier': 7, 'cat': 8, 'chair': 9, 'dog': 10,
    'fire_hydrant': 11, 'kiosk': 12, 'motorcycle': 13, 'movable_signage': 14,
    'parking_meter': 15, 'person': 16, 'pole': 17, 'potted_plant': 18,
    'power_controller': 19, 'scooter': 20, 'stop': 21, 'stroller': 22,
    'table': 23, 'traffic_light': 24, 'traffic_light_controller': 25,
    'traffic_sign': 26, 'tree_trunk': 27, 'truck': 28, 'wheelchair': 29
}
NUM_LABELS = len(CLASS_TO_IDX)

# ==============================================================================
# --- ðŸ¤– ëª¨ë¸ í´ëž˜ìŠ¤ ì •ì˜ (Model Definition) ---
# ==============================================================================
# ì²« ë²ˆì§¸ ì½”ë“œì—ì„œ ìž˜ ìž‘ë™í–ˆë˜ ëª¨ë¸ í´ëž˜ìŠ¤ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
class DirectSegFormer(nn.Module):
    def __init__(self, pretrained_model_name="nvidia/mit-b0", num_classes=30):
        super().__init__()
        self.original_model = SegformerForSemanticSegmentation.from_pretrained(
            pretrained_model_name,
            num_labels=num_classes,
            ignore_mismatched_sizes=True,
            trust_remote_code=True,
            torch_dtype=torch.float32,
            use_safetensors=True
        )
    def forward(self, x):   
        outputs = self.original_model(pixel_values=x)
        return outputs.logits

# ==============================================================================
# --- ðŸ“ ROS2 ë…¸ë“œ í´ëž˜ìŠ¤ (ROS2 Node Class) ---
# ==============================================================================
class SemanticBEVNode(Node):
    def __init__(self):
        super().__init__('semantic_bev_node')
        
        # --- ë³€ìˆ˜ ì´ˆê¸°í™” ---
        self.latest_rgb_image = None
        self.latest_depth_image = None
        self.device = DEVICE
        self.bridge = CvBridge()
        self.get_logger().info(f"Using device: {self.device}")

        # --- ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ë¡œë”© (ì²« ë²ˆì§¸ ì½”ë“œ ê¸°ë°˜) ---
        self.model = self.load_model()
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])

        # --- ì»¬ëŸ¬ë§µ ìƒì„± ---
        self.color_palette_2d = self.create_color_palette_2d() # 2D ë§ˆìŠ¤í¬ìš©
        self.color_palette_bev = self.create_color_palette_bev() # BEV ë§µìš©

        # --- ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„° ë° ë³€í™˜ í–‰ë ¬ ê³„ì‚° ---
        self.depth_intrinsics = np.array([
            [CAMERA_PARAMS['depth']['fx'], 0, CAMERA_PARAMS['depth']['cx']],
            [0, CAMERA_PARAMS['depth']['fy'], CAMERA_PARAMS['depth']['cy']],
            [0, 0, 1]
        ])
        self.extrinsic_hmt = self.create_hmt(CAM_TRANSLATION, CAM_QUATERNION)

        # --- ROS2 êµ¬ë… ë° íƒ€ì´ë¨¸ ì„¤ì • ---
        self.rgb_sub = self.create_subscription(Image, RGB_TOPIC, self.rgb_callback, 10)
        self.depth_sub = self.create_subscription(Image, DEPTH_TOPIC, self.depth_callback, 10)
        self.timer = self.create_timer(0.1, self.process_and_visualize) # 10Hz
        self.get_logger().info('Semantic BEV node started. Waiting for images...')

    # --- ëª¨ë¸ ë¡œë”© í•¨ìˆ˜ (ì²« ë²ˆì§¸ ì½”ë“œì™€ ë™ì¼) ---
    def load_model(self):
        model = DirectSegFormer(num_classes=NUM_LABELS)
        try:
            checkpoint = torch.load(MODEL_PATH, map_location=self.device)
            new_state_dict = {}
            # state_dict í‚¤ ì´ë¦„ì´ ë‹¬ë¼ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë¡œì§
            for key, value in checkpoint.items():
                if key.startswith('segformer.') or key.startswith('decode_head.'):
                    new_key = 'original_model.' + key
                    new_state_dict[new_key] = value
                else:
                    new_state_dict[key] = value
            model.load_state_dict(new_state_dict, strict=False)
            self.get_logger().info(f"Model loaded successfully from '{MODEL_PATH}'")
        except Exception as e:
            self.get_logger().error(f"Model loading failed: {e}. Using pre-trained weights.")
        model.to(self.device)
        model.eval()
        return model

    # --- ì»¬ëŸ¬ë§µ ìƒì„± í•¨ìˆ˜ë“¤ ---
    def create_color_palette_2d(self):
        cmap = plt.cm.get_cmap('jet', NUM_LABELS)
        palette = np.zeros((NUM_LABELS, 3), dtype=np.uint8)
        for i in range(NUM_LABELS):
            if i == 0: palette[i] = [0, 0, 0]
            else:
                rgba = cmap(i)
                palette[i] = (int(rgba[2] * 255), int(rgba[1] * 255), int(rgba[0] * 255))
        return palette

    def create_color_palette_bev(self):
        cmap = plt.cm.get_cmap('hsv', NUM_LABELS)
        palette = np.zeros((NUM_LABELS + 1, 3), dtype=np.uint8) # +1 for generic obstacle
        for i in range(NUM_LABELS):
            if i == 0: palette[i] = [0, 0, 0] # Background: Black
            else:
                rgba = cmap(i / NUM_LABELS)
                palette[i] = (int(rgba[2] * 255), int(rgba[1] * 255), int(rgba[0] * 255))
        return palette

    # --- ì´ë¯¸ì§€ ì½œë°± í•¨ìˆ˜ë“¤ ---
    def rgb_callback(self, msg):
        try:
            self.latest_rgb_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
        except Exception as e:
            self.get_logger().error(f"Failed to convert RGB image: {e}")

    def depth_callback(self, msg):
        try:
            depth_mm = self.bridge.imgmsg_to_cv2(msg, "16UC1")
            self.latest_depth_image = depth_mm.astype(np.float32) / 1000.0
        except Exception as e:
            self.get_logger().error(f"Failed to convert depth image: {e}")

    # --- ë©”ì¸ ì²˜ë¦¬ ë° ì‹œê°í™” í•¨ìˆ˜ ---
    def process_and_visualize(self):
        if self.latest_rgb_image is None or self.latest_depth_image is None:
            return

        img_bgr = self.latest_rgb_image.copy()
        depth_image = self.latest_depth_image.copy()
        original_h, original_w, _ = img_bgr.shape

        # 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° Semantic Mask ì¶”ë¡ 
        resized_img = cv2.resize(img_bgr, INFERENCE_SIZE, interpolation=cv2.INTER_LINEAR)
        rgb_for_model = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)
        input_tensor = self.transform(rgb_for_model).unsqueeze(0).to(self.device)

        with torch.no_grad():
            logits = self.model(input_tensor)
        
        upsampled_logits = F.interpolate(logits, size=(original_h, original_w), mode='bilinear', align_corners=False)
        semantic_mask = torch.argmax(upsampled_logits, dim=1).squeeze().cpu().numpy().astype(np.uint8)

        # 2. Semantic BEV ë§µ ìƒì„±
        bev_map = self.create_semantic_bev(depth_image, semantic_mask)

        # 3. ì‹œê°í™”
        # 2D Semantic Mask ì‹œê°í™”
        mask_colored = self.color_palette_2d[semantic_mask]
        overlay_2d = cv2.addWeighted(img_bgr, 0.6, mask_colored, 0.4, 0)

        # BEV ë§µ ì‹œê°í™”
        bev_colored = self.color_palette_bev[bev_map]
        
        # ë¡œë´‡ ìœ„ì¹˜ í‘œì‹œ (ì¤‘ì•™ í•˜ë‹¨)
        bev_h, bev_w, _ = bev_colored.shape
        cv2.drawMarker(bev_colored, (bev_w // 2, bev_h -1), (0, 0, 255), cv2.MARKER_TILTED_CROSS, 20, 3)

        # ê²°ê³¼ ì´ë¯¸ì§€ ë³‘í•© ë° í‘œì‹œ
        # 2D ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ BEV ë§µ í¬ê¸°ì— ë§žì¶¤ (500x500)
        display_size = (bev_h, bev_w)
        overlay_resized = cv2.resize(overlay_2d, display_size)
        
        # ì¢Œ: 2D ì˜¤ë²„ë ˆì´, ìš°: BEV ë§µ
        final_display = np.hstack((overlay_resized, bev_colored))
        
        cv2.imshow("Semantic BEV", final_display)
        key = cv2.waitKey(1) & 0xFF
        if key == 27: # ESC
            self.destroy_node()
            cv2.destroyAllWindows()
            rclpy.shutdown()

    # --- BEV ìƒì„± ê´€ë ¨ í•¨ìˆ˜ë“¤ ---
    def create_semantic_bev(self, depth_image, semantic_mask):
        # 1. 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„± (ì¹´ë©”ë¼ ì¢Œí‘œê³„)
        h, w = depth_image.shape
        u, v = np.meshgrid(np.arange(w), np.arange(h))
        
        valid = depth_image > 0
        z_cam = np.where(valid, depth_image, 0)
        x_cam = (u - self.depth_intrinsics[0, 2]) * z_cam / self.depth_intrinsics[0, 0]
        y_cam = (v - self.depth_intrinsics[1, 2]) * z_cam / self.depth_intrinsics[1, 1]
        
        points_cam = np.stack((x_cam, y_cam, z_cam), axis=-1).reshape(-1, 3)
        
        # 2. Semantic Maskë¥¼ Depth ì´ë¯¸ì§€ í¬ê¸°ì— ë§žê²Œ ë¦¬ì‚¬ì´ì¦ˆ (ìµœê·¼ì ‘ ì´ì›ƒ ë³´ê°„)
        mask_resized = cv2.resize(semantic_mask, (w, h), interpolation=cv2.INTER_NEAREST)
        labels = mask_resized.flatten()
        
        # 3. ìœ íš¨í•œ í¬ì¸íŠ¸ë§Œ í•„í„°ë§ (Depth ê°’ì´ ìžˆê³ , ë°°ê²½ì´ ì•„ë‹Œ í¬ì¸íŠ¸)
        # âœ¨ í•µì‹¬ ìˆ˜ì •: ë°°ê²½(ID=0)ì¸ í¬ì¸íŠ¸ëŠ” BEV ë§µì— ê·¸ë¦¬ì§€ ì•ŠìŒ
        valid_indices = (valid.flatten()) & (labels != 0)
        
        points_cam_valid = points_cam[valid_indices]
        labels_valid = labels[valid_indices]
        
        # 4. ì¹´ë©”ë¼ ì¢Œí‘œê³„ -> ë¡œë´‡ ë² ì´ìŠ¤ ì¢Œí‘œê³„ë¡œ ë³€í™˜
        homo_points = np.hstack((points_cam_valid, np.ones((points_cam_valid.shape[0], 1))))
        points_robot = (self.extrinsic_hmt @ homo_points.T).T[:, :3]

        # 5. ë¡œë´‡ ê¸°ì¤€ ë†’ì´ í•„í„°ë§ (ë°”ë‹¥, ì²œìž¥ ë“± ì œì™¸)
        height_filter = (points_robot[:, 2] > Z_MIN_ROBOT) & (points_robot[:, 2] < Z_MAX_ROBOT)
        points_filtered = points_robot[height_filter]
        labels_filtered = labels_valid[height_filter]

        # 6. 3D í¬ì¸íŠ¸ë¥¼ 2D BEV ê·¸ë¦¬ë“œë¡œ íˆ¬ì˜
        bev_pixel_size = int(BEV_SIZE_M / BEV_RESOLUTION)
        bev_map = np.zeros((bev_pixel_size, bev_pixel_size), dtype=np.uint8)
        
        x_robot, y_robot = points_filtered[:, 0], points_filtered[:, 1]
        
        # ë¡œë´‡ ì¢Œí‘œ(X: ì „ë°©, Y: ì¢Œì¸¡) -> BEV ì´ë¯¸ì§€ ì¢Œí‘œ(v: ì•„ëž˜, u: ìš°ì¸¡)
        u_bev = (bev_pixel_size // 2 - y_robot / BEV_RESOLUTION).astype(int)
        v_bev = (bev_pixel_size - 1 - x_robot / BEV_RESOLUTION).astype(int)
        
        # BEV ë§µ ê²½ê³„ ë‚´ì— ìžˆëŠ” ìœ íš¨í•œ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
        valid_bev_indices = (u_bev >= 0) & (u_bev < bev_pixel_size) & \
                            (v_bev >= 0) & (v_bev < bev_pixel_size)
        
        bev_map[v_bev[valid_bev_indices], u_bev[valid_bev_indices]] = labels_filtered[valid_bev_indices]
        
        return bev_map
        
    def create_hmt(self, translation, quaternion):
        # ë™ì°¨ ë³€í™˜ í–‰ë ¬ (Homogeneous Transformation Matrix) ìƒì„±
        rot_matrix = quat2mat([quaternion[3], quaternion[0], quaternion[1], quaternion[2]]) # w, x, y, z
        hmt = np.eye(4)
        hmt[:3, :3] = rot_matrix
        hmt[:3, 3] = translation
        return hmt

# ==============================================================================
# --- ðŸ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (Main Execution) ---
# ==============================================================================
def main(args=None):
    rclpy.init(args=args)
    node = SemanticBEVNode()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        if rclpy.ok():
            node.destroy_node()
            rclpy.shutdown()

if __name__ == '__main__':
    main()
