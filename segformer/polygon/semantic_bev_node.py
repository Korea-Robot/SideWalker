#!/usr/bin/env python3

# ==============================================================================
# --- ðŸš€ Imports ---
# ==============================================================================
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import SegformerForSemanticSegmentation
import numpy as np
import math
import os
from transforms3d.quaternions import quat2mat
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt
from PIL import Image as PILImage
from torchvision.transforms import v2 as T

# ==============================================================================
# --- ðŸš€ Configuration ---
# ==============================================================================
MODEL_PATH = 'best_model2.pth'  # í›ˆë ¨ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ê²½ë¡œ
NUM_LABELS = 30
GENERIC_OBSTACLE_ID = NUM_LABELS

# ==============================================================================
# --- ðŸš€ Class Definitions, Labels, and Colormap ---
# ==============================================================================
# (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
class_to_idx = {
    'background': 0, 'barricade': 1, 'bench': 2, 'bicycle': 3, 'bollard': 4,
    'bus': 5, 'car': 6, 'carrier': 7, 'cat': 8, 'chair': 9, 'dog': 10,
    'fire_hydrant': 11, 'kiosk': 12, 'motorcycle': 13, 'movable_signage': 14,
    'parking_meter': 15, 'person': 16, 'pole': 17, 'potted_plant': 18,
    'power_controller': 19, 'scooter': 20, 'stop': 21, 'stroller': 22,
    'table': 23, 'traffic_light': 24, 'traffic_light_controller': 25,
    'traffic_sign': 26, 'tree_trunk': 27, 'truck': 28, 'wheelchair': 29
}
id2label = {idx: label for label, idx in class_to_idx.items()}

def create_bev_colormap(num_base_classes, generic_obstacle_id):
    """BEV ì‹œê°í™”ë¥¼ ìœ„í•œ ì»¬ëŸ¬ë§µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    # Matplotlibì˜ 'hsv' ì»¬ëŸ¬ë§µì„ ê¸°ë°˜ìœ¼ë¡œ ìƒ‰ìƒ ìƒì„±
    hues = np.linspace(0.0, 1.0, num_base_classes, endpoint=False)
    colors = [plt.cm.hsv(h) for h in hues]
    # ë°°ê²½(ID 0)ì€ ê²€ì€ìƒ‰ìœ¼ë¡œ
    colors[0] = (0, 0, 0, 1)
    # ì¼ë°˜ ìž¥ì• ë¬¼(ID 30)ì€ í°ìƒ‰ìœ¼ë¡œ
    colors.append((1.0, 1.0, 1.0, 1.0))
    # ì»¬ëŸ¬ë§µ ê°ì²´ ìƒì„±
    cmap_mpl = ListedColormap(colors)


    # OpenCVì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìžˆë„ë¡ (B, G, R) í˜•íƒœì˜ ë£©ì—… í…Œì´ë¸”ë¡œ ë³€í™˜
    # modified 31 colors => 256 size table extended 
    full_colormap_cv = np.zeros((256,3),dtype=np.uint8) # 256 size black table generate 

    # # â—ï¸ ìˆ˜ì •ëœ ë¶€ë¶„: listë¥¼ np.arrayë¡œ ë³€í™˜ í›„ ìŠ¬ë¼ì´ì‹±
    # # OpenCVì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìžˆë„ë¡ (B, G, R) í˜•íƒœì˜ ë£©ì—… í…Œì´ë¸”ë¡œ ë³€í™˜
    colors_np = np.array(cmap_mpl.colors) # ë¦¬ìŠ¤íŠ¸ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜
    num_defined_colors = len(colors_np)
    # colormap_cv = (colors_np[:, :3] * 255).astype(np.uint8)[:, ::-1] # RGB to BGR

    # return colormap_cv

    # ì •ì˜ëœ ìƒ‰ìƒ(31ê°œ)ì„ ë³µì‚¬
    defined_colors_bgr = (colors_np[:, :3] * 255).astype(np.uint8)[:, ::-1] # RGB to BGR
    full_colormap_cv[:num_defined_colors] = defined_colors_bgr

    return full_colormap_cv

# ==============================================================================
# --- ðŸš€ Geometric and Camera Functions ---
# ==============================================================================
# (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
def intrinsics_from_fov(w, h, fov_h_deg, fov_v_deg):
    fov_h, fov_v = math.radians(fov_h_deg), math.radians(fov_v_deg)
    fx = w / (2.0 * math.tan(fov_h / 2.0))
    fy = h / (2.0 * math.tan(fov_v / 2.0))
    return np.array([[fx, 0.0, w / 2.0], [0.0, fy, h / 2.0], [0.0, 0.0, 1.0]])

# RealSense D435 ì¹´ë©”ë¼ì˜ ì¼ë°˜ì ì¸ ì‚¬ì–‘ì— ë§žì¶˜ ê°’
RGB_INTRINSICS = intrinsics_from_fov(640, 480, 69.0, 42.0)
DEPTH_INTRINSICS = intrinsics_from_fov(640, 480, 87.0, 58.0)

def create_hmt(translation, quaternion):
    rot_matrix = quat2mat([quaternion[3], quaternion[0], quaternion[1], quaternion[2]])
    hmt = np.eye(4); hmt[:3, :3] = rot_matrix; hmt[:3, 3] = translation
    return hmt

EXTRINSIC_HMT = create_hmt([-0.015, 0.22, 0.05], [0.49, -0.51, 0.5, -0.5])

# ==============================================================================
# --- ðŸš€ SegFormer Model Definition ---
# ==============================================================================
class SegFormer(nn.Module):
    def __init__(self, num_classes=30):
        super().__init__()
        self.model = SegformerForSemanticSegmentation.from_pretrained(
            "nvidia/mit-b0", num_labels=num_classes, id2label=id2label,
            label2id={l: i for i, l in id2label.items()},
            ignore_mismatched_sizes=True, torch_dtype=torch.float32,use_safetensors=True
        )
    def forward(self, x):
        return self.model(pixel_values=x).logits


# ==============================================================================
# --- ðŸš€ ROS2 Node Class ---
# ==============================================================================
class RealtimeSemanticBEVNode(Node):
    def __init__(self):
        super().__init__('realtime_semantic_bev_node')
        
        # --- ROS2 Setup ---
        self.bridge = CvBridge()
        self.rgb_sub = self.create_subscription(Image, '/camera/camera/color/image_raw', self.rgb_callback, 10)
        self.depth_sub = self.create_subscription(Image, '/camera/camera/depth/image_rect_raw', self.depth_callback, 10)
        
        # --- Member Variables ---
        self.latest_rgb_image = None
        self.latest_depth_image = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.get_logger().info(f"Using device: {self.device}")

        # --- AI Model and Preprocessing ---
        self.model = self._load_model(MODEL_PATH, NUM_LABELS).to(self.device).eval()
        self.transform = T.Compose([
            T.ToImage(),
            T.ToDtype(torch.float32, scale=True),
            T.Resize((224, 224), antialias=True),
            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])

        # --- Visualization ---
        self.bev_colormap_cv = create_bev_colormap(NUM_LABELS, GENERIC_OBSTACLE_ID)
        
        # --- Main Processing Loop ---
        self.timer = self.create_timer(0.1, self.process_and_visualize) # 10 Hz

    def _load_model(self, model_path, num_classes):
        """ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜."""
        model = SegFormer(num_classes=num_classes)
        if os.path.exists(model_path):
            try:
                checkpoint = torch.load(model_path, map_location=self.device)
                state_dict = checkpoint.get('state_dict', checkpoint)
                new_state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}
                model.load_state_dict(new_state_dict, strict=False)
                self.get_logger().info(f"âœ… Successfully loaded model weights from '{model_path}'.")
            except Exception as e:
                self.get_logger().warn(f"âš ï¸ Model loading error: {e}. Using pre-trained weights.")
        else:
            self.get_logger().warn(f"âš ï¸ Model file '{model_path}' not found. Using pre-trained weights.")
        return model

    def rgb_callback(self, msg):
        """RGB ì´ë¯¸ì§€ í† í”½ì„ ìˆ˜ì‹ í•˜ëŠ” ì½œë°± í•¨ìˆ˜."""
        try:
            # ROS Image ë©”ì‹œì§€ë¥¼ OpenCV BGR í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            self.latest_rgb_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
        except Exception as e:
            self.get_logger().error(f"Failed to process RGB image: {e}")

    def depth_callback(self, msg):
        """Depth ì´ë¯¸ì§€ í† í”½ì„ ìˆ˜ì‹ í•˜ëŠ” ì½œë°± í•¨ìˆ˜."""
        try:
            # 16UC1 í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (mm ë‹¨ìœ„) í›„ ë¯¸í„° ë‹¨ìœ„ë¡œ ë³€ê²½
            depth_mm = self.bridge.imgmsg_to_cv2(msg, "16UC1")
            self.latest_depth_image = (depth_mm / 1000.0).astype(np.float32)
        except Exception as e:
            self.get_logger().error(f"Failed to process depth image: {e}")

    def process_and_visualize(self):
        """ì£¼ê¸°ì ìœ¼ë¡œ í˜¸ì¶œë˜ì–´ BEV ìƒì„± ë° ì‹œê°í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜."""
        if self.latest_rgb_image is None or self.latest_depth_image is None:
            self.get_logger().info("Waiting for RGB and Depth images...")
            return

        # --- 1. Semantic Mask ìƒì„± ---
        # OpenCV(BGR) ì´ë¯¸ì§€ë¥¼ ëª¨ë¸ ìž…ë ¥ì— ë§žê²Œ RGBë¡œ ë³€í™˜
        rgb_for_model = cv2.cvtColor(self.latest_rgb_image, cv2.COLOR_BGR2RGB)
        semantic_mask = self._predict_semantic_mask(rgb_for_model)
        
        # --- 2. Semantic BEV ìƒì„± ---
        semantic_bev = self._create_semantic_bev(self.latest_depth_image, semantic_mask)

        # --- 3. ì‹œê°í™” ---
        # Maskì™€ BEVì— ì»¬ëŸ¬ë§µ ì ìš©
        mask_colored = cv2.applyColorMap((semantic_mask * (255 // NUM_LABELS)).astype(np.uint8), cv2.COLORMAP_JET)
        # bev_colored = cv2.LUT(cv2.cvtColor(semantic_bev, cv2.COLOR_GRAY2BGR), self.bev_colormap_cv)
        bev_colored = cv2.LUT(semantic_bev,self.bev_colormap_cv)

        # ë¡œë´‡ ìœ„ì¹˜ í‘œì‹œ
        bev_h, bev_w, _ = bev_colored.shape
        cv2.drawMarker(bev_colored, (bev_w // 2, bev_h -1), (0, 0, 255), cv2.MARKER_TILTED_CROSS, 20, 3)

        # ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì˜ ì°½ì— í‘œì‹œí•˜ê¸° ìœ„í•´ í¬ê¸° ì¡°ì ˆ ë° ë³‘í•©
        h, w, _ = self.latest_rgb_image.shape
        mask_resized = cv2.resize(mask_colored, (w, h))
        bev_resized = cv2.resize(bev_colored, (w, h))
        
        top_row = np.hstack((self.latest_rgb_image, mask_resized))
        # BEVë¥¼ ì•„ëž˜ìª½ì— í¬ê²Œ í‘œì‹œí•˜ê¸° ìœ„í•´ ë¹ˆ ê³µê°„ ìƒì„±
        bottom_row_placeholder = np.zeros_like(top_row)
        bev_large = cv2.resize(bev_colored, (top_row.shape[1], top_row.shape[0]))
        
        # ì œëª© ì¶”ê°€
        cv2.putText(self.latest_rgb_image, "RGB Image", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        cv2.putText(mask_resized, "Semantic Mask", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        cv2.putText(bev_large, "Semantic BEV", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)

        final_display = np.vstack((np.hstack((self.latest_rgb_image, mask_resized)), bev_large))

        cv2.imshow("Realtime Semantic BEV", final_display)
        cv2.waitKey(1)
        
    def _predict_semantic_mask(self, rgb_image_np):
        """RGB ì´ë¯¸ì§€ë¡œ Semantic Maskë¥¼ ì¶”ë¡ í•˜ëŠ” í•¨ìˆ˜."""
        with torch.no_grad():
            input_tensor = self.transform(rgb_image_np).unsqueeze(0).to(self.device)
            logits = self.model(input_tensor)
            
            # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì—…ìƒ˜í”Œë§
            upsampled_logits = F.interpolate(
                logits, size=rgb_image_np.shape[:2], mode='bilinear', align_corners=False
            )
            predictions = torch.argmax(upsampled_logits, dim=1)
            return predictions[0].cpu().numpy()

    def _create_semantic_bev(self, depth_image, semantic_mask, bev_resolution=0.05, bev_size_m=10.0, z_min=0.2, z_max=1.5):
        """BEV ë§µì„ ìƒì„±í•˜ëŠ” í•µì‹¬ ë¡œì§."""
        # 1. Depth ì´ë¯¸ì§€ì—ì„œ 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„± (ì¹´ë©”ë¼ ì¢Œí‘œê³„)
        points_cam, valid_mask_depth = self._unproject_depth(depth_image, DEPTH_INTRINSICS)
        
        # 2. ìœ íš¨í•œ 3D í¬ì¸íŠ¸ì— í•´ë‹¹í•˜ëŠ” Semantic Label ì°¾ê¸°
        h_rgb, w_rgb = semantic_mask.shape
        # semantic_maskë¥¼ depth ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        semantic_mask_resized = np.array(PILImage.fromarray(semantic_mask.astype(np.uint8)).resize((depth_image.shape[1], depth_image.shape[0]), PILImage.NEAREST))
        
        semantic_labels = semantic_mask_resized.flatten()[valid_mask_depth]
        valid_points_cam = points_cam[valid_mask_depth]
        
        # 3. í¬ì¸íŠ¸ í´ë¼ìš°ë“œë¥¼ ì¹´ë©”ë¼ ì¢Œí‘œê³„ì—ì„œ ë¡œë´‡ ë² ì´ìŠ¤ ì¢Œí‘œê³„ë¡œ ë³€í™˜
        points_robot = self._apply_transform(valid_points_cam, EXTRINSIC_HMT)

        # 4. ë¡œë´‡ ê¸°ì¤€ ë†’ì´(z)ë¡œ í¬ì¸íŠ¸ í•„í„°ë§ (ë°”ë‹¥, ì²œìž¥ ë“± ì œì™¸)
        height_filter = (points_robot[:, 2] > z_min) & (points_robot[:, 2] < z_max)
        points_filtered = points_robot[height_filter]
        labels_filtered = semantic_labels[height_filter]

        # 5. ë°°ê²½(ID 0)ìœ¼ë¡œ ë¶„ë¥˜ëœ í¬ì¸íŠ¸ë¥¼ 'ì¼ë°˜ ìž¥ì• ë¬¼'ë¡œ ìž¬ì§€ì •
        labels_for_bev = np.where(labels_filtered == 0, GENERIC_OBSTACLE_ID, labels_filtered)

        # 6. í•„í„°ë§ëœ 3D í¬ì¸íŠ¸ë¥¼ 2D BEV ê·¸ë¦¬ë“œë¡œ íˆ¬ì˜
        bev_pixel_size = int(bev_size_m / bev_resolution)
        bev_image = np.zeros((bev_pixel_size, bev_pixel_size), dtype=np.uint8)
        
        x_robot, y_robot = points_filtered[:, 0], points_filtered[:, 1]
        
        # ë¡œë´‡ ì¢Œí‘œ(X: ì „ë°©, Y: ì¢Œì¸¡) -> BEV ì´ë¯¸ì§€ ì¢Œí‘œ(v: ì•„ëž˜, u: ìš°ì¸¡)
        u_bev = (bev_pixel_size // 2 - y_robot / bev_resolution).astype(int)
        v_bev = (bev_pixel_size - 1 - x_robot / bev_resolution).astype(int)
        
        valid_indices = (u_bev >= 0) & (u_bev < bev_pixel_size) & (v_bev >= 0) & (v_bev < bev_pixel_size)
        
        bev_image[v_bev[valid_indices], u_bev[valid_indices]] = labels_for_bev[valid_indices]
        
        return bev_image

    def _unproject_depth(self, depth_map, K):
        fx, fy, cx, cy = K[0, 0], K[1, 1], K[0, 2], K[1, 2]
        h, w = depth_map.shape
        u, v = np.meshgrid(np.arange(w), np.arange(h))
        
        valid = depth_map > 0
        z = np.where(valid, depth_map, 0)
        x = (u - cx) * z / fx
        y = (v - cy) * z / fy
        
        points_3d = np.stack((x, y, z), axis=-1)
        return points_3d.reshape(-1, 3), valid.flatten()

    def _apply_transform(self, points, hmt):
        homo_points = np.hstack((points, np.ones((points.shape[0], 1))))
        transformed = homo_points @ hmt.T
        return transformed[:, :3]

# ==============================================================================
# --- ðŸš€ Main Execution ---
# ==============================================================================
def main(args=None):
    rclpy.init(args=args)
    node = RealtimeSemanticBEVNode()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
