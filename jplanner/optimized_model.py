#!/usr/bin/env python3
"""
Semantic Segmentation Models for Point Cloud Reconstruction
"""

import numpy as np
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms
from PIL import Image as PILImage
from transformers import (
    SegformerForSemanticSegmentation,
    AutoImageProcessor,
    # MaskFormerForInstanceSegmentation
    Mask2FormerForUniversalSegmentation
)
from torch.cuda.amp import autocast ### MODIFIED: Import autocast

class CustomSegFormer(nn.Module):
    """Custom trained SegFormer model"""
    
    def __init__(self, num_classes: int = 30, pretrained_name: str = "nvidia/mit-b0"):
        super().__init__()
        try:
            self.model = SegformerForSemanticSegmentation.from_pretrained(
                pretrained_name,
                num_labels=config.num_custom_classes,
                ignore_mismatched_sizes=True,
                trust_remote_code=True,
                torch_dtype=torch.float32,
                use_safetensors=True,
            )
        except ValueError as e:
            if "torch.load" in str(e):
                print(f"Warning: {e}")
                print("Creating model architecture without pretrained weights...")
                from transformers import SegformerConfig
                config = SegformerConfig.from_pretrained(pretrained_name)
                config.num_labels = num_classes
                self.model = SegformerForSemanticSegmentation(config)
            else:
                raise e
    
    def forward(self, x):
        outputs = self.model(pixel_values=x)
        return outputs.logits


class SemanticModel:
    """Unified interface for different semantic segmentation models"""
    
    def __init__(self, config, device, logger=None):
        self.config = config
        self.device = device
        self.logger = logger
        self.model = None
        self.image_processor = None

        ### MODIFIED: Add flags for optimization ###
        self.enable_half = (self.device.type == 'cuda')
        # (H, W) for transforms/logic
        self.inference_size_hw = (config.inference_size, config.inference_size) 
        # (W, H) for cv2.resize
        self.inference_size_wh = (config.inference_size, config.inference_size) 
        
        if not config.use_semantic:
            self._log("Semantic segmentation disabled - using RGB only")
            return
        
        self.model_type = config.model_type
        self._load_model()
        
        if self.enable_half:
            self._log("âš¡ Half Precision (FP16) enabled for inference")
    
    def _log(self, msg):
        if self.logger:
            self.logger.info(msg)
        else:
            print(msg)
    
    def _load_model(self):
        """Load the specified model"""
        if self.model_type == "custom-object":
            self._load_custom_model()
        elif self.model_type == "custom-surface":
            self._load_custom_model()
        elif self.model_type == "segformer-cityscapes":
            self._load_segformer()
        elif self.model_type == "maskformer-cityscapes":
            self._load_maskformer()
        else:
            raise ValueError(f"Unknown model type: {self.model_type}")
    
    def _load_custom_model(self):
        """Load custom trained SegFormer"""
        self.model = CustomSegFormer(num_classes=self.config.num_custom_classes)
        
        try:
            checkpoint = torch.load(
                self.config.custom_model_path, 
                map_location=self.device,
                weights_only=False
            )
            
            # Map checkpoint keys
            new_state_dict = {}
            for key, value in checkpoint.items():
                if key.startswith('segformer.') or key.startswith('decode_head.'):
                    new_key = 'model.' + key
                else:
                    new_key = key
                new_state_dict[new_key] = value
            
            self.model.load_state_dict(new_state_dict, strict=False)
            self._log(f"âœ… Custom model loaded from {self.config.custom_model_path}")
            
        except Exception as e:
            self._log(f"âš ï¸ Model loading failed: {e}")
            self._log("Using model without pretrained weights")
        
        self.model.to(self.device)
        self.model.eval()

        ### MODIFIED: Remove Resize from transform, as we pre-resize ###
        self.transform = transforms.Compose([
            # transforms.Resize((self.config.inference_size, self.config.inference_size)), # Removed
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])
    
    def _load_segformer(self):
        """Load SegFormer model"""

        model_name = self.config.active_model_name

        self.image_processor = AutoImageProcessor.from_pretrained(
            model_name, #self.config.segformer_checkpoint,
        )
        self.model = SegformerForSemanticSegmentation.from_pretrained(
            model_name, #self.config.segformer_checkpoint,
            # num_labels=self.config.num_custom_classes
            ignore_mismatched_sizes=True,
            trust_remote_code=True,
            torch_dtype=torch.float32,
            use_safetensors=True,            
        )
        self.model.to(self.device)
        self.model.eval()
        self._log("âœ… SegFormer model loaded")
    
    def _load_maskformer(self):
        """Load MaskFormer model"""

        model_name = self.config.active_model_name
        self.image_processor = AutoImageProcessor.from_pretrained(
            model_name, #self.config.maskformer_checkpoint
        )
        # self.model = MaskFormerForInstanceSegmentation.from_pretrained(
        self.model = Mask2FormerForUniversalSegmentation.from_pretrained(
            model_name, #self.config.maskformer_checkpoint
        )
        self.model.to(self.device)
        self.model.eval()
        self._log("âœ… MaskFormer-COCO model loaded")
    
    def predict(self, rgb_image):
        """
        Run semantic segmentation on RGB image
        
        Args:
            rgb_image: BGR image (OpenCV format)
            
        Returns:
            semantic_mask: (H, W) numpy array with class labels, or None if semantic disabled
        """
        if not self.config.use_semantic:
            return None
        
        if self.model_type == "custom-object":
            return self._predict_custom(rgb_image)
        elif self.model_type == "custom-surface":
            return self._predict_custom(rgb_image)
        elif self.model_type == "segformer-cityscapes":
            return self._predict_segformer(rgb_image)
        elif self.model_type == "maskformer-cityscapes":
            return self._predict_maskformer(rgb_image)
    
    def _predict_custom(self, rgb_image):
        """Custom model inference"""
        
        ### MODIFIED: Pre-resize image ###
        h_orig, w_orig = rgb_image.shape[:2]
        if (h_orig, w_orig) != self.inference_size_hw:
            rgb_image_resized = cv2.resize(
                rgb_image, 
                self.inference_size_wh, # (W, H) for cv2
                interpolation=cv2.INTER_LINEAR
            )
        else:
            rgb_image_resized = rgb_image
        
        # BGR -> RGB
        rgb_image_rgb = cv2.cvtColor(rgb_image_resized, cv2.COLOR_BGR2RGB)
        pil_image = PILImage.fromarray(rgb_image_rgb)
        
        # Preprocess
        input_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)
        
        # Inference
        with torch.no_grad():
            ### MODIFIED: Use autocast for FP16 ###
            with autocast(enabled=self.enable_half):
                logits = self.model(input_tensor)
            
            # Cast back to float32 for interpolation safety
            if self.enable_half:
                logits = logits.float()
        
        ### MODIFIED: Upsample to original size ###
        logits = F.interpolate(
            logits,
            size=(h_orig, w_orig), # Use original shape
            mode='bilinear',
            align_corners=False
        )
        
        # Get class predictions
        pred_mask = torch.argmax(logits, dim=1).squeeze()
        return pred_mask.cpu().numpy().astype(np.uint8)
    
    def _predict_segformer(self, rgb_image):
        """SegFormer-ADE20k inference"""

        ### MODIFIED: Pre-resize image ###
        h_orig, w_orig = rgb_image.shape[:2]
        if (h_orig, w_orig) != self.inference_size_hw:
            rgb_image_resized = cv2.resize(
                rgb_image, 
                self.inference_size_wh, # (W, H) for cv2
                interpolation=cv2.INTER_LINEAR
            )
        else:
            rgb_image_resized = rgb_image
        
        rgb_image_rgb = cv2.cvtColor(rgb_image_resized, cv2.COLOR_BGR2RGB)
        pil_image = PILImage.fromarray(rgb_image_rgb)
        
        inputs = self.image_processor(images=pil_image, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            ### MODIFIED: Use autocast for FP16 ###
            with autocast(enabled=self.enable_half):
                outputs = self.model(**inputs)
        
        ### MODIFIED: Cast back to float32 for post-processing ###
        if self.enable_half:
            outputs.logits = outputs.logits.float()

        result = self.image_processor.post_process_semantic_segmentation(
            outputs,
            target_sizes=[(h_orig, w_orig)] ### MODIFIED: Use original shape
        )[0]
        
        return result.cpu().numpy().astype(np.uint8)
    
    def _predict_maskformer(self, rgb_image):
        """MaskFormer-COCO inference"""

        ### MODIFIED: Pre-resize image ###
        h_orig, w_orig = rgb_image.shape[:2]
        if (h_orig, w_orig) != self.inference_size_hw:
            rgb_image_resized = cv2.resize(
                rgb_image, 
                self.inference_size_wh, # (W, H) for cv2
                interpolation=cv2.INTER_LINEAR
            )
        else:
            rgb_image_resized = rgb_image
            
        rgb_image_rgb = cv2.cvtColor(rgb_image_resized, cv2.COLOR_BGR2RGB)
        pil_image = PILImage.fromarray(rgb_image_rgb)
        
        inputs = self.image_processor(images=pil_image, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            ### MODIFIED: Use autocast for FP16 ###
            with autocast(enabled=self.enable_half):
                outputs = self.model(**inputs)
        
        ### MODIFIED: Cast back to float32 for post-processing ###
        if self.enable_half:
            # MaskFormer has multiple outputs, cast all relevant ones
            if outputs.class_queries_logits is not None:
                outputs.class_queries_logits = outputs.class_queries_logits.float()
            if outputs.masks_queries_logits is not None:
                outputs.masks_queries_logits = outputs.masks_queries_logits.float()

        # result = self.image_processor.post_process_panoptic_segmentation(
        result = self.image_processor.post_process_semantic_segmentation(
            outputs,
            target_sizes=[(h_orig, w_orig)] ### MODIFIED: Use original shape
        )[0]
        
        return result.cpu().numpy().astype(np.uint8)
        # return result["segmentation"].cpu().numpy().astype(np.uint8)

"""
ë„¤! ì´ë²ˆì— ì ìš©ëœ ìµœì í™”ëŠ” `SemanticModel`ì˜ ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ëŠ” ë° ì´ˆì ì„ ë§ì¶˜, ë§¤ìš° íš¨ê³¼ì ì¸ ë‘ ê°€ì§€ ê¸°ë²•ì…ë‹ˆë‹¤.

ì–´ë–¤ ë¶€ë¶„ì´ ë°”ë€Œì—ˆê³  ì™œ ê·¸ë ‡ê²Œ í–ˆëŠ”ì§€ í•˜ë‚˜í•˜ë‚˜ ì„¤ëª…í•´ ë“œë¦´ê²Œìš”.

---

## 1. âš¡ï¸ Half Precision (FP16) ì¶”ë¡  ì ìš©

**"ì—°ì‚° ì •ë°€ë„ë¥¼ ë‚®ì¶°ì„œ ì†ë„ë¥¼ 2ë°° ê°€ê¹Œì´ ì˜¬ë¦½ë‹ˆë‹¤."**

* **WHAT (ë¬´ì—‡ì´ ë°”ë€Œì—ˆë‚˜?)**
    1.  `from torch.cuda.amp import autocast`ë¥¼ ì„í¬íŠ¸í–ˆìŠµë‹ˆë‹¤.
    2.  `__init__` í•¨ìˆ˜ì— `self.enable_half = (self.device.type == 'cuda')`ë¼ëŠ” í”Œë˜ê·¸ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. (GPUë¥¼ ì‚¬ìš©í•  ë•Œë§Œ í™œì„±í™”ë©ë‹ˆë‹¤.)
    3.  ëª¨ë“  `_predict_...` í•¨ìˆ˜ì˜ `with torch.no_grad():` ë‚´ë¶€ì— `with autocast(enabled=self.enable_half):` ë¸”ë¡ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.
    4.  ëª¨ë¸ ì¶”ë¡ (`self.model(...)`)ì´ ì´ `autocast` ë¸”ë¡ ì•ˆì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.
    5.  `autocast` ë¸”ë¡ ì§í›„, `logits.float()`ì²˜ëŸ¼ ëª¨ë¸ì˜ ì¶œë ¥ê°’ì„ ë‹¤ì‹œ `.float()` (FP32)ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.

* **WHY (ì™œ ë°”ë€Œì—ˆë‚˜?)**
    * **ì†ë„ í–¥ìƒ:** `autocast`ëŠ” PyTorchì˜ **Automatic Mixed Precision** ê¸°ëŠ¥ì…ë‹ˆë‹¤. ì´ ë¸”ë¡ ì•ˆì—ì„œ ì‹¤í–‰ë˜ëŠ” ì—°ì‚°(íŠ¹íˆ CNN, Linear ë ˆì´ì–´)ì€ ê¸°ë³¸ ì •ë°€ë„ì¸ **FP32 (32ë¹„íŠ¸)** ëŒ€ì‹  **FP16 (16ë¹„íŠ¸)**ìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤.
    * NVIDIA GPUì˜ **Tensor Core**ëŠ” FP16 ì—°ì‚°ì— íŠ¹í™”ë˜ì–´ ìˆì–´, FP32 ì—°ì‚°ë³´ë‹¤ í›¨ì”¬ ë¹ ë¦…ë‹ˆë‹¤. (ì´ë¡ ì ìœ¼ë¡œ ìµœëŒ€ 2ë°° ì´ìƒ)
    * **ë©”ëª¨ë¦¬ ì ˆì•½:** ëª¨ë¸ ê°€ì¤‘ì¹˜ì™€ ì¤‘ê°„ ê³„ì‚° ê°’ë“¤ì´ ì‚¬ìš©í•˜ëŠ” GPU ë©”ëª¨ë¦¬ë„ ì ˆë°˜ìœ¼ë¡œ ì¤„ì–´ë“­ë‹ˆë‹¤.
    * **ì•ˆì •ì„±:** `logits.float()`ë¡œ ë‹¤ì‹œ ë³€í™˜í•˜ëŠ” ì´ìœ ëŠ”, `F.interpolate`ë‚˜ `argmax` ê°™ì€ í›„ì† ì—°ì‚°ì´ CPUì—ì„œ ì‹¤í–‰ë˜ê±°ë‚˜ FP32 ì •ë°€ë„ë¥¼ ìš”êµ¬í•  ë•Œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ì„œì…ë‹ˆë‹¤. ê°€ì¥ ë¬´ê±°ìš´ ì—°ì‚°(ëª¨ë¸ ì¶”ë¡ )ì€ ì´ë¯¸ FP16ìœ¼ë¡œ ë¹ ë¥´ê²Œ ëë‚œ ìƒíƒœì…ë‹ˆë‹¤.

---

## 2. ğŸ–¼ï¸ ì…ë ¥ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ (ì „ì²˜ë¦¬)

**"ëª¨ë¸ì— ì‘ì€ ì´ë¯¸ì§€ë¥¼ ì…ë ¥í•´ì„œ ì—°ì‚°ëŸ‰ì„ íšê¸°ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤."**

* **WHAT (ë¬´ì—‡ì´ ë°”ë€Œì—ˆë‚˜?)**
    1.  `__init__` í•¨ìˆ˜ì— `self.inference_size_hw` (ë†’ì´, ë„ˆë¹„)ì™€ `self.inference_size_wh` (ë„ˆë¹„, ë†’ì´) ë³€ìˆ˜ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. (OpenCVëŠ” `(W, H)` ìˆœì„œë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ëª…í™•íˆ êµ¬ë¶„)
    2.  ëª¨ë“  `_predict_...` í•¨ìˆ˜ê°€ ì‹œì‘ë  ë•Œ, ì›ë³¸ ì´ë¯¸ì§€ì˜ í¬ê¸°(`h_orig`, `w_orig`)ë¥¼ ë¨¼ì € ì €ì¥í•©ë‹ˆë‹¤.
    3.  `cv2.resize` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´, ì›ë³¸ ì´ë¯¸ì§€ë¥¼ `self.inference_size_wh` (ì˜ˆ: 512x512) í¬ê¸°ë¡œ **ë¨¼ì € ì¶•ì†Œ**í•©ë‹ˆë‹¤. (`rgb_image_resized` ìƒì„±)
    4.  `cv2.cvtColor`, `PILImage.fromarray`, `self.transform` (ë˜ëŠ” `image_processor`) ë“± ëª¨ë“  ì „ì²˜ë¦¬ê°€ ì´ **ì¶•ì†Œëœ ì´ë¯¸ì§€**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤.

* **WHY (ì™œ ë°”ë€Œì—ˆë‚˜?)**
    * **ë³‘ëª© í˜„ìƒ ì œê±°:** RealSenseì—ì„œ ë“¤ì–´ì˜¤ëŠ” 1280x720 (ì•½ 92ë§Œ í”½ì…€) ë˜ëŠ” 1920x1080 (ì•½ 207ë§Œ í”½ì…€) ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ê·¸ëŒ€ë¡œ ëª¨ë¸ì— ì…ë ¥í•˜ëŠ” ê²ƒì€ ì—„ì²­ë‚œ ë³‘ëª©ì…ë‹ˆë‹¤.
    * **ì—°ì‚°ëŸ‰ ê°ì†Œ:** ëª¨ë¸ì˜ ì—°ì‚°ëŸ‰(FLOPs)ì€ ì…ë ¥ í•´ìƒë„ì— ë¹„ë¡€(ë˜ëŠ” ê·¸ ì´ìƒ)í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ 512x512 (ì•½ 26ë§Œ í”½ì…€)ë¡œ ì¤„ì—¬ì„œ ì…ë ¥í•˜ë©´, ëª¨ë¸ì´ ì²˜ë¦¬í•  í”½ì…€ ìˆ˜ê°€ **1/3 ~ 1/8** ìˆ˜ì¤€ìœ¼ë¡œ ì¤„ì–´ë“¤ì–´ ì¶”ë¡  ì†ë„ê°€ ì••ë„ì ìœ¼ë¡œ ë¹¨ë¼ì§‘ë‹ˆë‹¤.

---

## 3. ğŸ“ˆ ë§ˆìŠ¤í¬ ì—…ìƒ˜í”Œë§ (í›„ì²˜ë¦¬)

**"ì‘ê²Œ ì˜ˆì¸¡ëœ ë§ˆìŠ¤í¬ë¥¼ ë‹¤ì‹œ ì›ë³¸ í¬ê¸°ë¡œ í™•ëŒ€í•©ë‹ˆë‹¤."**

* **WHAT (ë¬´ì—‡ì´ ë°”ë€Œì—ˆë‚˜?)**
    1.  `_predict_custom`ì˜ `F.interpolate` í•¨ìˆ˜ì˜ `size` ì¸ìë¥¼ `(h_orig, w_orig)` (ì›ë³¸ í¬ê¸°)ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.
    2.  `_predict_segformer`, `_predict_maskformer`ì˜ `post_process_...` í•¨ìˆ˜ì˜ `target_sizes` ì¸ìë¥¼ `[(h_orig, w_orig)]` (ì›ë³¸ í¬ê¸°)ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.

* **WHY (ì™œ ë°”ë€Œì—ˆë‚˜?)**
    * **í•´ìƒë„ ì¼ì¹˜:** 2ë²ˆì—ì„œ ìš°ë¦¬ëŠ” **ì‘ì€ ì´ë¯¸ì§€**ë¡œ ì¶”ë¡ í–ˆê¸° ë•Œë¬¸ì—, ê²°ê³¼ë¬¼ë¡œ ë‚˜ì˜¨ ë§ˆìŠ¤í¬ë„ **ì‘ì€ í¬ê¸°**ì…ë‹ˆë‹¤.
    * í•˜ì§€ë§Œ `ReconstructionNode`ì˜ `_align_to_depth` í•¨ìˆ˜ëŠ” **ì›ë³¸ ì´ë¯¸ì§€**ì™€ ë™ì¼í•œ í•´ìƒë„ì˜ ë§ˆìŠ¤í¬ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤.
    * ë”°ë¼ì„œ ì¶”ë¡  ì§í›„, ëª¨ë¸ì´ ì¶œë ¥í•œ ì‘ì€ ë§ˆìŠ¤í¬(ì˜ˆ: 512x512)ë¥¼ `F.interpolate` (bilinear ë³´ê°„) ë˜ëŠ” `post_process` ê¸°ëŠ¥ì„ ì´ìš©í•´ ë‹¤ì‹œ ì›ë³¸ í¬ê¸°(ì˜ˆ: 1280x720)ë¡œ **í™•ëŒ€**í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.

---

## 4. ğŸ§¹ ê¸°íƒ€ ì •ë¦¬ (ì¤‘ë³µ ì œê±°)

* **WHAT (ë¬´ì—‡ì´ ë°”ë€Œì—ˆë‚˜?)**
    * `_load_custom_model`ì˜ `self.transform` (PyTorch `transforms.Compose`) ë¦¬ìŠ¤íŠ¸ì—ì„œ `transforms.Resize(...)` ë‹¨ê³„ë¥¼ **ì œê±°**í–ˆìŠµë‹ˆë‹¤.

* **WHY (ì™œ ë°”ë€Œì—ˆë‚˜?)**
    * **ì¤‘ë³µ ì œê±°:** 2ë²ˆì—ì„œ `cv2.resize`ë¥¼ ì‚¬ìš©í•´ ì´ë¯¸ì§€ë¥¼ ë¯¸ë¦¬ ë¦¬ì‚¬ì´ì¦ˆí•˜ê¸° ë•Œë¬¸ì—, PyTorchì˜ `transforms.Resize`ê°€ ë˜ ì‹¤í–‰ë  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. (OpenCVì˜ `cv2.resize`ê°€ PILì„ ì´ìš©í•˜ëŠ” `transforms.Resize`ë³´ë‹¤ ì¼ë°˜ì ìœ¼ë¡œ ë” ë¹ ë¦…ë‹ˆë‹¤.)

---

## ğŸ¯ ê²°ë¡ 

ì´ ìµœì í™”ë“¤ì„ í†µí•´ `semantic_model.predict()` í•¨ìˆ˜ëŠ” ì´ì œ ë‹¤ìŒê³¼ ê°™ì´ ë™ì‘í•©ë‹ˆë‹¤.

1.  **[Pre-process]** í° (720p) BGR ì´ë¯¸ì§€ë¥¼ ë°›ìŒ.
2.  `cv2.resize`ë¡œ ì‘ì€ (512x512) ì´ë¯¸ì§€ë¡œ ì¶•ì†Œ (ì†ë„ í–¥ìƒ â¬†ï¸).
3.  **[Inference]** `autocast`ë¥¼ ì‚¬ìš©í•´ FP16ìœ¼ë¡œ ëª¨ë¸ ì¶”ë¡  (ì†ë„ í–¥ìƒ â¬†ï¸, ë©”ëª¨ë¦¬ ê°ì†Œ â¬‡ï¸).
4.  **[Post-process]** ì‘ì€ (512x512) ë§ˆìŠ¤í¬ ê²°ê³¼ë¥¼ ì–»ìŒ.
5.  `F.interpolate`ë¡œ í° (720p) ë§ˆìŠ¤í¬ë¡œ í™•ëŒ€.
6.  `ReconstructionNode`ë¡œ ìµœì¢… ë§ˆìŠ¤í¬ ë°˜í™˜.

ê²°ê³¼ì ìœ¼ë¡œ **ì‹œë§¨í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ ë‹¨ê³„ì˜ ì†ë„ê°€ í¬ê²Œ í–¥ìƒ**ë˜ì–´, `rgbd_callback`ì˜ ì „ì²´ì ì¸ FPS(ì´ˆë‹¹ í”„ë ˆì„ ìˆ˜)ê°€ ì˜¬ë¼ê°€ê²Œ ë©ë‹ˆë‹¤.
"""
