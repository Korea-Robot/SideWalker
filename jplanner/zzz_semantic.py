#!/usr/bin/env python3

### problem

"""

1. RGB + Depth image get
2. Semantic Model Use => Tensor RT & NPU use 
3. Depth 3D point cloud : inverse projection => CPU to GPU optimize
4. semantic + 3D point cloud  => Resolution & depth Error 
5. BEV map 
6. ros2 topic publish  

Message Synchronization & depth accuracy

Process all GPU -> CPU 

rgb camera info topic


# Optimization 

1. Preprocess 
- RGB decrease resolution
- Depth down sampling
- GPU data transfer : pinned memory use 

2. Semantic Segmentation speed up 
- Latency
- FPS 
- ONNX, TensorRT, TorchScript

3. Depth -> Pointcloud 
- GPU based transform 

4. Semantic + BEV vectorize
- loop removal
- GPU process

5. ROS Publish optimize 
- compressed image, publish rate limit, background thread 


---

header:

  stamp:

    sec: 1762930499

    nanosec: 303927246

  frame_id: camera_color_optical_frame

height: 480

width: 640

distortion_model: plumb_bob

d:

- -0.05512524023652077

- 0.06190275400876999

- -0.00023902612156234682

- -0.00012431867071427405

- -0.019336095079779625

k:

- 385.97442626953125

- 0.0

- 322.1943359375

- 0.0

- 385.46087646484375

- 238.75344848632812

- 0.0

- 0.0

- 1.0

r:

- 1.0

- 0.0

- 0.0

- 0.0

- 1.0

- 0.0

- 0.0

- 0.0

- 1.0

p:

- 385.97442626953125

- 0.0

- 322.1943359375

- 0.0

- 0.0

- 385.46087646484375

- 238.75344848632812

- 0.0

- 0.0

- 0.0

- 1.0

- 0.0

binning_x: 0

binning_y: 0

roi:

  x_offset: 0

  y_offset: 0

  height: 0

  width: 0

  do_rectify: false

---



depth intrinsic info topic



header:

  stamp:

    sec: 1762930594

    nanosec: 974354492

  frame_id: camera_depth_optical_frame

height: 480

width: 640

distortion_model: plumb_bob

d:

- 0.0

- 0.0

- 0.0

- 0.0

- 0.0

k:

- 395.630859375

- 0.0

- 324.56903076171875

- 0.0

- 395.630859375

- 242.35031127929688

- 0.0

- 0.0

- 1.0

r:

- 1.0

- 0.0

- 0.0

- 0.0

- 1.0

- 0.0

- 0.0

- 0.0

- 1.0

p:

- 395.630859375

- 0.0

- 324.56903076171875

- 0.0

- 0.0

- 395.630859375

- 242.35031127929688

- 0.0

- 0.0

- 0.0

- 1.0

- 0.0

binning_x: 0

binning_y: 0

roi:

  x_offset: 0

  y_offset: 0

  height: 0

  width: 0

  do_rectify: false

---

^C



ros2 topic echo /camera/camera/extrinsics/depth_to_color

1762930640.389163 [123]       ros2: config: //CycloneDDS/Domain/General: 'NetworkInterfaceAddress': deprecated element (file:///home/krm/.cyclonedds.xml line 8)

rotation:

- 0.9999944567680359

- 0.0004453109868336469

- -0.003304719226434827

- -0.00045781597145833075

- 0.9999927282333374

- -0.0037841906305402517

- 0.003303010016679764

- 0.0037856826093047857

- 0.9999873638153076

translation:

- -0.05908159539103508

- 1.4681237189506646e-05

- 0.00048153731040656567

---



ros2 topic echo /camera/camera/extrinsics/depth_to_depth

1762930669.043295 [123]       ros2: config: //CycloneDDS/Domain/General: 'NetworkInterfaceAddress': deprecated element (file:///home/krm/.cyclonedds.xml line 8)

rotation:

- 1.0

- 0.0

- 0.0

- 0.0

- 1.0

- 0.0

- 0.0

- 0.0

- 1.0

translation:

- 0.0

- 0.0

- 0.0

---




ê²° ë°©ì•ˆ
ì´ ë¬¸ì œë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ëª‡ ê°€ì§€ ë‹¨ê³„ê°€ ìˆìŠµë‹ˆë‹¤.

1. (ê°€ì¥ ì‰¬ìš´ í•´ê²°ì±…) Realsenseì˜ 'Aligned' í† í”½ ì‚¬ìš©
Intel Realsense ì¹´ë©”ë¼ëŠ” depth_to_color extrinsicsë¥¼ ì´ìš©í•´ Depth ì´ë¯¸ì§€ë¥¼ RGB ì¹´ë©”ë¼ ì‹œì ìœ¼ë¡œ ë³€í™˜(Warping)ì‹œí‚¨ í† í”½ì„ ì´ë¯¸ ë°œí–‰í•´ ì¤„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.

Depth í† í”½ ë³€ê²½: depth_topicì„ /camera/camera/depth/image_rect_raw (í˜„ì¬ ê°’) ëŒ€ì‹  /camera/camera/aligned_depth_to_color/image_rawë¡œ ë³€ê²½í•©ë‹ˆë‹¤.

Intrinsics í†µì¼: ì´ aligned_depth_to_color ì´ë¯¸ì§€ëŠ” RGB ì¹´ë©”ë¼ì˜ ì‹œì ê³¼ í•´ìƒë„(640x480)ë¥¼ ë”°ë¦…ë‹ˆë‹¤. ë”°ë¼ì„œ depth_to_pointcloud_gpu í•¨ìˆ˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” Intrinsics(fx, fy, cx, cy)ëŠ” **Depth ì¹´ë©”ë¼ ê°’(395.63...)ì´ ì•„ë‹Œ RGB ì¹´ë©”ë¼ ê°’(385.97...)**ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

ì •ë ¬ ì½”ë“œ ì œê±°: RGBì™€ Aligned DepthëŠ” ì´ë¯¸ ê°™ì€ ì‹œì ê³¼ í•´ìƒë„(640x480)ë¥¼ ê°€ì§€ë¯€ë¡œ, rgbd_callbackì˜ Step 3. GPU ê¸°ë°˜ ì •ë ¬ (F.interpolate) ë¶€ë¶„ì´ í†µì§¸ë¡œ í•„ìš” ì—†ì–´ì§‘ë‹ˆë‹¤.

íŒŒë¼ë¯¸í„° ìˆ˜ì •: ì½”ë“œì˜ depth_cam ë° rgb_cam í•´ìƒë„ì™€ Intrinsics íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë‘ ì‹¤ì œ RGB í† í”½(640x480, fx=385.97...) ê°’ìœ¼ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤.

2. (Aligned í† í”½ì´ ì—†ì„ ì‹œ) ìˆ˜ë™ ì •ë ¬ êµ¬í˜„
ë§Œì•½ aligned_depth_to_color í† í”½ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤ë©´, F.interpolateë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹  ì§ì ‘ 3D-to-2D í”„ë¡œì ì…˜ì„ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.

ì˜¬ë°”ë¥¸ í† í”½ êµ¬ë…:

RGB: /camera/camera/color/image_rect_raw (ì™œê³¡ ë³´ì •ëœ ì´ë¯¸ì§€)

Depth: /camera/camera/depth/image_rect_raw (ì™œê³¡ ë³´ì •ëœ ì´ë¯¸ì§€)

ì˜¬ë°”ë¥¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©: ì½”ë“œì˜ ëª¨ë“  Intrinsicsì™€ í•´ìƒë„ ê°’ì„ ì‹¤ì œ ROS í† í”½(640x480) ê°’ìœ¼ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤.

ì •ë ¬ ë¡œì§ ë³€ê²½: F.interpolate ëŒ€ì‹  ë‹¤ìŒ ë¡œì§ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

a. depth_to_pointcloud_gpuë¥¼ (ì˜¬ë°”ë¥¸ Depth Intrinsicsë¡œ) í˜¸ì¶œí•˜ì—¬ camera_depth_optical_frame ê¸°ì¤€ 3D í¬ì¸íŠ¸ P_d (H, W, 3)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

b. depth_to_color Extrinsics í–‰ë ¬ T_c_dë¥¼ apply_transform_gpu í•¨ìˆ˜ì— ì ìš©í•˜ì—¬ P_dë¥¼ camera_color_optical_frame ê¸°ì¤€ 3D í¬ì¸íŠ¸ P_cë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

c. P_c (x, y, z)ë¥¼ **RGB ì¹´ë©”ë¼ì˜ Intrinsics(P í–‰ë ¬)**ë¥¼ ì‚¬ìš©í•´ 2D í”½ì…€ ì¢Œí‘œ (u_c, v_c)ë¡œ í”„ë¡œì ì…˜í•©ë‹ˆë‹¤.

d. (u_c, v_c) ì¢Œí‘œë¥¼ ì´ìš©í•´ RGB ì´ë¯¸ì§€ì™€ ì‹œë§¨í‹± ë§ˆìŠ¤í¬ì—ì„œ í•´ë‹¹ í”½ì…€ì˜ ìƒ‰ìƒê³¼ ë¼ë²¨ì„ ìƒ˜í”Œë§í•©ë‹ˆë‹¤. (ì´ ê³¼ì •ì„ torch.nn.functional.grid_sampleë¡œ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)

ì œ ìƒê°ì—ëŠ” í•´ê²° ë°©ì•ˆ 1ì´ Realsense ì¹´ë©”ë¼ì˜ í‘œì¤€ ê¸°ëŠ¥ì„ í™œìš©í•˜ëŠ” ê°€ì¥ íš¨ìœ¨ì ì´ê³  ì •í™•í•œ ë°©ë²•ì…ë‹ˆë‹¤.



"""

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, PointCloud2, PointField
from std_msgs.msg import Header
import numpy as np
import cv2
from cv_bridge import CvBridge
from tf2_ros import Buffer, TransformListener, TransformException
from transforms3d.quaternions import quat2mat, mat2quat
from transforms3d.affines import compose
from rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy
import message_filters # ë™ê¸°í™”ë¥¼ ìœ„í•´
import torch
import torch.nn.functional as F # GPU ê¸°ë°˜ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§• (Alignment)
import time
from collections import deque

# ============================================================================
# ì˜ì¡´ì„± ì½”ë“œ (Config)
# (ì œê³µëœ 'reconstruction_config.py' ë‚´ìš©ì„ ì—¬ê¸°ì— ë¶™ì—¬ë„£ìŠµë‹ˆë‹¤)
# ============================================================================
from dataclasses import dataclass, field
from typing import Optional, Dict

# 1. Custom Object Classes
OBJECT_CLASSES = {
    'background': 0, 'barricade': 1, 'bench': 2, 'bicycle': 3,
    'bollard': 4, 'bus': 5, 'car': 6, 'carrier': 7, 'cat': 8,
    'chair': 9, 'dog': 10, 'fire_hydrant': 11, 'kiosk': 12,
    'motorcycle': 13, 'movable_signage': 14, 'parking_meter': 15,
    'person': 16, 'pole': 17, 'potted_plant': 18,
    'power_controller': 19, 'scooter': 20, 'stop': 21,
    'stroller': 22, 'table': 23, 'traffic_light': 24,
    'traffic_light_controller': 25, 'traffic_sign': 26,
    'tree_trunk': 27, 'truck': 28, 'wheelchair': 29
}
# 2. Custom Surface Classes
SURFACE_CLASSES = {
    'background': 0, 'caution_zone': 1, 'bike_lane': 2, 'alley': 3,
    'roadway': 4, 'braille_block': 5, 'sidewalk': 6
}
# 3. Cityscapes Classes (Segformer, Maskformer ê³µí†µ ì‚¬ìš©)
CITYSCAPES_CLASSES = {
    "road": 0, "sidewalk": 1, "building": 2, "wall": 3,
    "fence": 4, "pole": 5, "traffic light": 6, "traffic sign": 7,
    "vegetation": 8, "terrain": 9, "sky": 10, "person": 11,
    "rider": 12, "car": 13, "truck": 14, "bus": 15, "train": 16,
    "motorcycle": 17, "bicycle": 18
}
@dataclass
class CameraIntrinsics:
    """Camera intrinsic parameters"""
    fx: float
    fy: float
    cx: float
    cy: float
@dataclass
class ReconstructionConfig:
    """Main configuration for point cloud reconstruction"""
    use_semantic: bool = True
    model_type: str ="maskformer-cityscapes"
    
    custom_object_model_path: str = "models/dynamic_object/best_model2.pth.pth"
    custom_surface_model_path: str = "models/surface/surface_mask_best_lrup.pt.pth"

    segformer_checkpoint: str = "nvidia/segformer-b0-finetuned-cityscapes-1024-1024"
    maskformer_checkpoint: str = "facebook/mask2former-swin-tiny-cityscapes-semantic"
    active_model_name: str = field(init=False)
    inference_size: int = field(init=False)
    custom_class_names: Dict[str, int] = field(init=False)
    
    # --- â¬‡ï¸ ìˆ˜ì •ëœ ë¶€ë¶„ (640x480 ì‹¤ì œ ê°’ ê¸°ì¤€) â¬‡ï¸ ---
    depth_intrinsics: CameraIntrinsics = field(default_factory=lambda: CameraIntrinsics(
        # K: [395.63, 0.0, 324.56, 0.0, 395.63, 242.35]
        fx=395.630859375, fy=395.630859375, cx=324.56903076171875, cy=242.35031127929688
    ))
    # RGB IntrinsicsëŠ” Rectified P í–‰ë ¬ ê¸°ì¤€
    # P: [385.97, 0.0, 322.19, 0.0, 0.0, 385.46, 238.75, 0.0]
    rgb_intrinsics: CameraIntrinsics = field(default_factory=lambda: CameraIntrinsics(
        fx=385.97442626953125, fy=385.46087646484375, cx=322.1943359375, cy=238.75344848632812
    ))
    
    # ì¤‘ìš”: image_raw ëŒ€ì‹  ì™œê³¡ ë³´ì •ëœ image_rect_raw ì‚¬ìš©
    depth_topic: str = '/camera/camera/depth/image_rect_raw'
    rgb_topic: str = '/camera/camera/color/image_raw' # <-- ìˆ˜ì •ë¨
    # --- â¬†ï¸ ìˆ˜ì •ëœ ë¶€ë¶„ â¬†ï¸ ---
    
    # pointcloud_topic: str = '/semantic_pointcloud' # ë…¸ë“œì—ì„œ ì¬ì •ì˜ë¨
    pointcloud_topic: str = '/pointcloud' # ë…¸ë“œì—ì„œ ì¬ì •ì˜ë¨
    # bev_topic: str = '/semantic_bev_map' # ë…¸ë“œì—ì„œ ì¬ì •ì˜ë¨
    bev_topic: str = '/bev_map' # ë…¸ë“œì—ì„œ ì¬ì •ì˜ë¨
    
    source_frame: str = 'camera_depth_optical_frame'
    target_frame: str = 'camera_link'
    downsample_y: int = 3 # 9
    downsample_x: int = 2 # 6
    sync_slop: float = 0.1
    use_gpu: bool = True
    def __post_init__(self):
        if self.model_type == "custom-object":
            self.active_model_name = self.custom_object_model_path
            self.custom_class_names = OBJECT_CLASSES.copy()
            self.inference_size = 512
        elif self.model_type == "custom-surface":
            self.active_model_name = self.custom_surface_model_path
            self.custom_class_names = SURFACE_CLASSES.copy()
            self.inference_size = 512
        elif self.model_type == "segformer-cityscapes":
            self.active_model_name = self.segformer_checkpoint
            self.custom_class_names = CITYSCAPES_CLASSES.copy()
            self.inference_size = 512
        elif self.model_type == "maskformer-cityscapes":
            self.active_model_name = self.maskformer_checkpoint
            self.custom_class_names = CITYSCAPES_CLASSES.copy()
            self.inference_size = 384
        else:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” model_typeì…ë‹ˆë‹¤: {self.model_type}")
    @property
    def num_custom_classes(self) -> int:
        return len(self.custom_class_names)
    @property
    def idx_to_class(self) -> Dict[int, str]:
        return {v: k for k, v in self.custom_class_names.items()}

# ============================================================================
# ì˜ì¡´ì„± ì½”ë“œ (Model)
# (ì œê³µëœ 'optimized_model.py' ë‚´ìš©ì„ ì—¬ê¸°ì— ë¶™ì—¬ë„£ìŠµë‹ˆë‹¤)
# ============================================================================
import torch.nn as nn
from torchvision import transforms
from PIL import Image as PILImage
from transformers import (
    SegformerForSemanticSegmentation,
    AutoImageProcessor,
    Mask2FormerForUniversalSegmentation
)
from torch.cuda.amp import autocast

class CustomSegFormer(nn.Module):
    """Custom trained SegFormer model"""
    def __init__(self, num_classes: int = 30, pretrained_name: str = "nvidia/mit-b0"):
        super().__init__()
        try:
            self.model = SegformerForSemanticSegmentation.from_pretrained(
                pretrained_name,
                num_labels=config.num_custom_classes,
                ignore_mismatched_sizes=True,
                trust_remote_code=True,
                torch_dtype=torch.float32,
                use_safetensors=True,
            )
        except ValueError as e:
            if "torch.load" in str(e):
                print(f"Warning: {e}")
                print("Creating model architecture without pretrained weights...")
                from transformers import SegformerConfig
                config = SegformerConfig.from_pretrained(pretrained_name)
                config.num_labels = num_classes
                self.model = SegformerForSemanticSegmentation(config)
            else:
                raise e
    def forward(self, x):
        outputs = self.model(pixel_values=x)
        return outputs.logits

class SemanticModel:
    """Unified interface for different semantic segmentation models"""
    def __init__(self, config, device, logger=None):
        self.config = config
        self.device = device
        self.logger = logger
        self.model = None
        self.image_processor = None
        self.enable_half = (self.device.type == 'cuda')
        self.inference_size_hw = (config.inference_size, config.inference_size)
        self.inference_size_wh = (config.inference_size, config.inference_size)
        if not config.use_semantic:
            self._log("Semantic segmentation disabled - using RGB only")
            return
        self.model_type = config.model_type
        self._load_model()
        if self.enable_half:
            self._log("âš¡ Half Precision (FP16) enabled for inference")
    def _log(self, msg):
        if self.logger:
            self.logger.info(msg)
        else:
            print(msg)
    def _load_model(self):
        """Load the specified model"""
        if self.model_type == "custom-object":
            self._load_custom_model()
        elif self.model_type == "custom-surface":
            self._load_custom_model()
        elif self.model_type == "segformer-cityscapes":
            self._load_segformer()
        elif self.model_type == "maskformer-cityscapes":
            self._load_maskformer()
        else:
            raise ValueError(f"Unknown model type: {self.model_type}")
    def _load_custom_model(self):
        """Load custom trained SegFormer"""
        self.model = CustomSegFormer(num_classes=self.config.num_custom_classes)
        try:
            checkpoint = torch.load(
                self.config.custom_model_path,
                map_location=self.device,
                weights_only=False
            )
            new_state_dict = {}
            for key, value in checkpoint.items():
                if key.startswith('segformer.') or key.startswith('decode_head.'):
                    new_key = 'model.' + key
                else:
                    new_key = key
                new_state_dict[new_key] = value
            self.model.load_state_dict(new_state_dict, strict=False)
            self._log(f"âœ… Custom model loaded from {self.config.custom_model_path}")
        except Exception as e:
            self._log(f"âš ï¸ Model loading failed: {e}")
            self._log("Using model without pretrained weights")
        self.model.to(self.device)
        self.model.eval()
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])
    def _load_segformer(self):
        """Load SegFormer model"""
        model_name = self.config.active_model_name
        self.image_processor = AutoImageProcessor.from_pretrained(model_name)
        self.model = SegformerForSemanticSegmentation.from_pretrained(
            model_name,
            ignore_mismatched_sizes=True,
            trust_remote_code=True,
            torch_dtype=torch.float32,
            use_safetensors=True,
        )
        self.model.to(self.device)
        self.model.eval()
        self._log("âœ… SegFormer model loaded")
    def _load_maskformer(self):
        """Load MaskFormer model"""
        model_name = self.config.active_model_name
        self.image_processor = AutoImageProcessor.from_pretrained(model_name)
        self.model = Mask2FormerForUniversalSegmentation.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
        self._log("âœ… MaskFormer-COCO model loaded")
    def predict(self, rgb_image):
        """Run semantic segmentation on RGB image (BGR OpenCV format)"""
        if not self.config.use_semantic:
            return None
        if self.model_type == "custom-object":
            return self._predict_custom(rgb_image)
        elif self.model_type == "custom-surface":
            return self._predict_custom(rgb_image)
        elif self.model_type == "segformer-cityscapes":
            return self._predict_segformer(rgb_image)
        elif self.model_type == "maskformer-cityscapes":
            return self._predict_maskformer(rgb_image)
    def _predict_custom(self, rgb_image):
        h_orig, w_orig = rgb_image.shape[:2]
        if (h_orig, w_orig) != self.inference_size_hw:
            rgb_image_resized = cv2.resize(
                rgb_image, self.inference_size_wh, interpolation=cv2.INTER_LINEAR
            )
        else:
            rgb_image_resized = rgb_image
        rgb_image_rgb = cv2.cvtColor(rgb_image_resized, cv2.COLOR_BGR2RGB)
        pil_image = PILImage.fromarray(rgb_image_rgb)
        input_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)
        with torch.no_grad():
            with autocast(enabled=self.enable_half):
                logits = self.model(input_tensor)
            if self.enable_half:
                logits = logits.float()
        logits = F.interpolate(
            logits, size=(h_orig, w_orig), mode='bilinear', align_corners=False
        )
        pred_mask = torch.argmax(logits, dim=1).squeeze()
        return pred_mask.cpu().numpy().astype(np.uint8)
    def _predict_segformer(self, rgb_image):
        h_orig, w_orig = rgb_image.shape[:2]
        if (h_orig, w_orig) != self.inference_size_hw:
            rgb_image_resized = cv2.resize(
                rgb_image, self.inference_size_wh, interpolation=cv2.INTER_LINEAR
            )
        else:
            rgb_image_resized = rgb_image
        rgb_image_rgb = cv2.cvtColor(rgb_image_resized, cv2.COLOR_BGR2RGB)
        pil_image = PILImage.fromarray(rgb_image_rgb)
        inputs = self.image_processor(images=pil_image, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        with torch.no_grad():
            with autocast(enabled=self.enable_half):
                outputs = self.model(**inputs)
        if self.enable_half:
            outputs.logits = outputs.logits.float()
        result = self.image_processor.post_process_semantic_segmentation(
            outputs, target_sizes=[(h_orig, w_orig)]
        )[0]
        return result.cpu().numpy().astype(np.uint8)
    def _predict_maskformer(self, rgb_image):
        h_orig, w_orig = rgb_image.shape[:2]
        if (h_orig, w_orig) != self.inference_size_hw:
            rgb_image_resized = cv2.resize(
                rgb_image, self.inference_size_wh, interpolation=cv2.INTER_LINEAR
            )
        else:
            rgb_image_resized = rgb_image
        rgb_image_rgb = cv2.cvtColor(rgb_image_resized, cv2.COLOR_BGR2RGB)
        pil_image = PILImage.fromarray(rgb_image_rgb)
        inputs = self.image_processor(images=pil_image, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        with torch.no_grad():
            with autocast(enabled=self.enable_half):
                outputs = self.model(**inputs)
        if self.enable_half:
            if outputs.class_queries_logits is not None:
                outputs.class_queries_logits = outputs.class_queries_logits.float()
            if outputs.masks_queries_logits is not None:
                outputs.masks_queries_logits = outputs.masks_queries_logits.float()
        result = self.image_processor.post_process_semantic_segmentation(
            outputs, target_sizes=[(h_orig, w_orig)]
        )[0]
        return result.cpu().numpy().astype(np.uint8)


# ============================================================================
# ğŸš€ ë©”ì¸ ë…¸ë“œ: SemanticPointCloudBEVNode
# ============================================================================

class SemanticPointCloudBEVNode(Node):
    """
    Depth, RGB ì´ë¯¸ì§€ë¥¼ ë™ê¸°í™”í•˜ì—¬ ìˆ˜ì‹ í•˜ê³ ,
    Semantic Segmentationì„ ìˆ˜í–‰í•œ ë’¤,
    GPU ê°€ì†ì„ í†µí•´ Semantic Point Cloudì™€ Semantic BEV Mapì„ ë°œí–‰í•˜ëŠ” ë…¸ë“œ.
    (Extrinsicsë¥¼ ê³ ë ¤í•œ Projective Alignment ìˆ˜í–‰)
    """

    def __init__(self):
        super().__init__('semantic_pointcloud_bev_node')

        # --- 1. ê¸°ë³¸ ëª¨ë“ˆ ë° ì„¤ì • ë¡œë“œ ---
        self.bridge = CvBridge()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ì œê³µëœ ReconstructionConfig ì‚¬ìš©
        self.config = ReconstructionConfig()
        
        self.get_logger().info(f'ğŸš€ CUDA GPU ê°€ì† í™œì„±í™” (PyTorch, {self.device})')
        if not self.config.use_semantic:
            self.get_logger().warn('ì‹œë§¨í‹± ëª¨ë“œê°€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. (config.use_semantic = False)')

        # ì‹œë§¨í‹± ëª¨ë¸ ë¡œë“œ
        self.semantic_model = SemanticModel(self.config, self.device, self.get_logger())

        # --- 2. ROS íŒŒë¼ë¯¸í„° ì„ ì–¸ (PCL + BEV + Semantic) ---
        
        # --- â¬‡ï¸ ìˆ˜ì •ëœ ë¶€ë¶„ (640x480 ë° image_rect_raw ê¸°ì¤€) â¬‡ï¸ ---
        self.declare_parameter('depth_topic', self.config.depth_topic)
        self.declare_parameter('rgb_topic', self.config.rgb_topic) # /camera/camera/color/image_rect_raw
        self.declare_parameter('source_frame', self.config.source_frame)
        self.declare_parameter('target_frame', self.config.target_frame)
        self.declare_parameter('sync_slop', self.config.sync_slop)

        # Depth ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° (PCL ì¬êµ¬ì„±ìš©, 640x480)
        self.declare_parameter('depth_cam.fx', self.config.depth_intrinsics.fx) # 395.63
        self.declare_parameter('depth_cam.fy', self.config.depth_intrinsics.fy) # 395.63
        self.declare_parameter('depth_cam.cx', self.config.depth_intrinsics.cx) # 324.56
        self.declare_parameter('depth_cam.cy', self.config.depth_intrinsics.cy) # 242.35
        self.declare_parameter('depth_cam.height', 480)
        # self.declare_parameter('depth_cam.width', 640)
        self.declare_parameter('depth_cam.width', 848)

        # RGB ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° (Projective Alignmentìš©, 640x480)
        self.declare_parameter('rgb_cam.fx', self.config.rgb_intrinsics.fx) # 385.97
        self.declare_parameter('rgb_cam.fy', self.config.rgb_intrinsics.fy) # 385.46
        self.declare_parameter('rgb_cam.cx', self.config.rgb_intrinsics.cx) # 322.19
        self.declare_parameter('rgb_cam.cy', self.config.rgb_intrinsics.cy) # 238.75
        self.declare_parameter('rgb_cam.height', 480)
        # self.declare_parameter('rgb_cam.width', 640)
        self.declare_parameter('rgb_cam.width', 848)
        # --- â¬†ï¸ ìˆ˜ì •ëœ ë¶€ë¶„ â¬†ï¸ ---

        # Semantic Point Cloud íŒŒë¼ë¯¸í„°
        self.declare_parameter('semantic_pointcloud_topic', '/semantic_pointcloud')
        self.declare_parameter('pcl.downsample_y', self.config.downsample_y)
        self.declare_parameter('pcl.downsample_x', self.config.downsample_x)

        # Semantic BEV íŒŒë¼ë¯¸í„°
        self.declare_parameter('semantic_bev_topic', '/semantic_bev_map')
        self.declare_parameter('bev.z_min', 0.15)
        self.declare_parameter('bev.z_max', 1.0)
        self.declare_parameter('bev.resolution', 0.1)
        self.declare_parameter('bev.size_x', 30.0)
        self.declare_parameter('bev.size_y', 30.0)
        self.declare_parameter('bev.ignore_labels', [0, 10]) 

        # --- 3. íŒŒë¼ë¯¸í„° ê°’ í• ë‹¹ ---
        # PCL/BEV ê³µí†µ
        depth_topic = self.get_parameter('depth_topic').value
        rgb_topic = self.get_parameter('rgb_topic').value
        self.source_frame = self.get_parameter('source_frame').value
        self.target_frame = self.get_parameter('target_frame').value
        sync_slop = self.get_parameter('sync_slop').value

        # Depth ì¹´ë©”ë¼ (PCL)
        self.fx_d = self.get_parameter('depth_cam.fx').value
        self.fy_d = self.get_parameter('depth_cam.fy').value
        self.cx_d = self.get_parameter('depth_cam.cx').value
        self.cy_d = self.get_parameter('depth_cam.cy').value
        self.depth_height = self.get_parameter('depth_cam.height').value
        self.depth_width = self.get_parameter('depth_cam.width').value
        self.depth_shape_hw = (self.depth_height, self.depth_width)

        # RGB ì¹´ë©”ë¼ (Alignment)
        self.fx_rgb = self.get_parameter('rgb_cam.fx').value
        self.fy_rgb = self.get_parameter('rgb_cam.fy').value
        self.cx_rgb = self.get_parameter('rgb_cam.cx').value
        self.cy_rgb = self.get_parameter('rgb_cam.cy').value
        self.rgb_height = self.get_parameter('rgb_cam.height').value
        self.rgb_width = self.get_parameter('rgb_cam.width').value
        self.rgb_shape_hw = (self.rgb_height, self.rgb_width)

        # PCL íŒŒë¼ë¯¸í„°
        semantic_pointcloud_topic = self.get_parameter('semantic_pointcloud_topic').value
        self.downsample_y = self.get_parameter('pcl.downsample_y').value
        self.downsample_x = self.get_parameter('pcl.downsample_x').value

        # BEV íŒŒë¼ë¯¸í„°
        semantic_bev_topic = self.get_parameter('semantic_bev_topic').value
        self.z_min = self.get_parameter('bev.z_min').value
        self.z_max = self.get_parameter('bev.z_max').value
        self.resolution = self.get_parameter('bev.resolution').value
        self.size_x = self.get_parameter('bev.size_x').value
        self.size_y = self.get_parameter('bev.size_y').value
        self.bev_ignore_labels = self.get_parameter('bev.ignore_labels').value

        # BEV ê·¸ë¦¬ë“œ ì„¤ì •
        self.cells_x = int(self.size_x / self.resolution)
        self.cells_y = int(self.size_y / self.resolution)
        self.grid_origin_x = -self.size_x / 2.0
        self.grid_origin_y = -self.size_y / 2.0

        # --- 4. ROS í†µì‹  ì„¤ì • ---
        qos_profile = QoSProfile(
            reliability=ReliabilityPolicy.RELIABLE,
            history=HistoryPolicy.KEEP_LAST,
            depth=5
        )

        depth_sub = message_filters.Subscriber(self, Image, depth_topic, qos_profile=qos_profile)
        rgb_sub = message_filters.Subscriber(self, Image, rgb_topic, qos_profile=qos_profile)

        self.sync = message_filters.ApproximateTimeSynchronizer(
            [depth_sub, rgb_sub],
            queue_size=10,
            slop=sync_slop
        )
        self.sync.registerCallback(self.rgbd_callback)

        self.sem_pc_pub = self.create_publisher(PointCloud2, semantic_pointcloud_topic, qos_profile)
        self.sem_bev_pub = self.create_publisher(PointCloud2, semantic_bev_topic, qos_profile)

        self.tf_buffer = Buffer()
        self.tf_listener = TransformListener(self.tf_buffer, self)

        # --- 5. Point Cloud í•„ë“œ ì •ì˜ (PCL/BEV ê³µí†µ) ---
        self.semantic_pointcloud_fields = [
            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),
            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),
            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),
            PointField(name='rgb', offset=12, datatype=PointField.FLOAT32, count=1),
            PointField(name='label', offset=16, datatype=PointField.UINT32, count=1),
        ]
        self.point_step_pcl = 20 # 4*4 + 4 = 20 bytes
        self.semantic_bev_fields = self.semantic_pointcloud_fields
        self.point_step_bev = self.point_step_pcl

        # --- 6. GPU íŒŒë¼ë¯¸í„° ì´ˆê¸°í™” ---
        self._init_gpu_parameters()
        self._init_semantic_colormap()

        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.timings = {
            'total': deque(maxlen=50),
            'semantic': deque(maxlen=50),
            'align_gpu': deque(maxlen=50),
            'depth_to_pc': deque(maxlen=50),
            'transform': deque(maxlen=50),
            'pcl_pub': deque(maxlen=50),
            'bev_pub': deque(maxlen=50),
        }
        self.last_report_time = time.time()

        self.get_logger().info('âœ… Semantic PointCloud + BEV Node initialized (Projective Alignment)')
        self.get_logger().info(f"  RGB Topic: {rgb_topic}")
        self.get_logger().info(f"  Depth Topic: {depth_topic}")
        self.get_logger().info(f"  PCL Topic: {semantic_pointcloud_topic}")
        self.get_logger().info(f"  BEV Topic: {semantic_bev_topic} (Label í•„ë“œ í¬í•¨)")

    def _init_gpu_parameters(self):
        """GPUì—ì„œ ì‚¬ìš©í•  íŒŒë¼ë¯¸í„° ë¯¸ë¦¬ ìƒì„± (ì½œë°± í•¨ìˆ˜ ë‚´ ë¶€í•˜ ê°ì†Œ)"""

        # 1. PCL ì¬êµ¬ì„±ì„ ìœ„í•œ í”½ì…€ ê·¸ë¦¬ë“œ (Depth ì¹´ë©”ë¼ ì¢Œí‘œê³„, 640x480)
        v, u = torch.meshgrid(
            torch.arange(self.depth_height, device=self.device, dtype=torch.float32),
            torch.arange(self.depth_width, device=self.device, dtype=torch.float32),
            indexing='ij'
        )
        self.u_grid_d = u
        self.v_grid_d = v
        self.fx_d_tensor = torch.tensor(self.fx_d, device=self.device, dtype=torch.float32)
        self.fy_d_tensor = torch.tensor(self.fy_d, device=self.device, dtype=torch.float32)
        self.cx_d_tensor = torch.tensor(self.cx_d, device=self.device, dtype=torch.float32)
        self.cy_d_tensor = torch.tensor(self.cy_d, device=self.device, dtype=torch.float32)

        # 2. Projective Alignmentë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° (RGB ì¹´ë©”ë¼, 640x480)
        self.fx_rgb_tensor = torch.tensor(self.fx_rgb, device=self.device, dtype=torch.float32)
        self.fy_rgb_tensor = torch.tensor(self.fy_rgb, device=self.device, dtype=torch.float32)
        self.cx_rgb_tensor = torch.tensor(self.cx_rgb, device=self.device, dtype=torch.float32)
        self.cy_rgb_tensor = torch.tensor(self.cy_rgb, device=self.device, dtype=torch.float32)

        # 3. ê³ ì •ëœ Extrinsics (Depth -> Color)
        # ì œê³µëœ 'depth_to_color' í† í”½ ê°’ (NumPy)
        rotation_flat = [
            0.9999944567680359, 0.0004453109868336469, -0.003304719226434827,
            -0.00045781597145833075, 0.9999927282333374, -0.0037841906305402517,
            0.003303010016679764, 0.0037856826093047857, 0.9999873638153076
        ]
        translation_vec = [-0.05908159539103508, 1.4681237189506646e-05, 0.00048153731040656567]
        
        T_color_from_depth_np = np.eye(4, dtype=np.float32)
        T_color_from_depth_np[:3, :3] = np.array(rotation_flat).reshape(3, 3)
        T_color_from_depth_np[:3, 3] = np.array(translation_vec)

        # GPU í…ì„œë¡œ ë³€í™˜
        self.T_color_from_depth_gpu = torch.from_numpy(T_color_from_depth_np).to(self.device)
        self.get_logger().info('ê³ ì • Extrinsics (T_color_from_depth) GPUì— ë¡œë“œ ì™„ë£Œ')

        # 4. BEV ìƒì„±ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° (GPU í…ì„œ)
        self.z_min_t = torch.tensor(self.z_min, device=self.device, dtype=torch.float32)
        self.z_max_t = torch.tensor(self.z_max, device=self.device, dtype=torch.float32)
        self.resolution_t = torch.tensor(self.resolution, device=self.device, dtype=torch.float32)
        self.grid_origin_x_t = torch.tensor(self.grid_origin_x, device=self.device, dtype=torch.float32)
        self.grid_origin_y_t = torch.tensor(self.grid_origin_y, device=self.device, dtype=torch.float32)
        self.bev_ignore_labels_t = torch.tensor(self.bev_ignore_labels, device=self.device, dtype=torch.long)
        self.bev_packed_flat = torch.full(
            (self.cells_y * self.cells_x,), 0, device=self.device, dtype=torch.int64
        )
        self.get_logger().info(f'GPU íŒŒë¼ë¯¸í„° ì´ˆê¸°í™” ì™„ë£Œ ({self.depth_height}x{self.depth_width})')

    def _init_semantic_colormap(self):
        """ì‹œë§¨í‹± ë¼ë²¨ì„ RGBë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ GPU ì»¬ëŸ¬ë§µ ìƒì„±"""
        num_classes = self.config.num_custom_classes
        cityscapes_palette = [
            [128, 64, 128], [244, 35, 232], [70, 70, 70], [102, 102, 156], [190, 153, 153],
            [153, 153, 153], [250, 170, 30], [220, 220, 0], [107, 142, 35], [152, 251, 152],
            [70, 130, 180], [220, 20, 60], [255, 0, 0], [0, 0, 142], [0, 0, 70],
            [0, 60, 100], [0, 80, 100], [0, 0, 230], [119, 11, 32]
        ]
        colors = torch.zeros((num_classes, 3), dtype=torch.uint8, device=self.device)
        for i, (name, idx) in enumerate(self.config.custom_class_names.items()):
            if i < len(cityscapes_palette):
                colors[idx] = torch.tensor(cityscapes_palette[i], dtype=torch.uint8, device=self.device)
            else:
                r, g, b = (i * 50) % 255, (i * 90) % 255, (i * 120) % 255
                colors[idx] = torch.tensor([r, g, b], dtype=torch.uint8, device=self.device)
        colors[0] = torch.tensor([0, 0, 0], dtype=torch.uint8, device=self.device)
        self.semantic_colormap_gpu = colors
        self.get_logger().info(f'GPU ì‹œë§¨í‹± ì»¬ëŸ¬ë§µ ìƒì„± ì™„ë£Œ ({num_classes} classes)')


    # --- â¬‡ï¸ (í•µì‹¬) ìƒˆë¡œìš´ ì •ë ¬ í•¨ìˆ˜ â¬‡ï¸ ---
    def project_points_to_rgb_grid(self, points_in_color_frame):
        """
        3D í¬ì¸íŠ¸(color frame ê¸°ì¤€, H, W, 3)ë¥¼ RGB 2D í”½ì…€ ê·¸ë¦¬ë“œë¡œ í”„ë¡œì ì…˜í•©ë‹ˆë‹¤.
        F.grid_sampleì„ ìœ„í•œ ì •ê·œí™”ëœ ì¢Œí‘œ(-1 ~ 1)ì™€ ìœ íš¨ ë§ˆìŠ¤í¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        X = points_in_color_frame[..., 0]
        Y = points_in_color_frame[..., 1]
        Z = points_in_color_frame[..., 2]

        # Z > 0 (ì¹´ë©”ë¼ ì•) ì¸ í¬ì¸íŠ¸ë§Œ ìœ íš¨
        z_mask = Z > 1e-6
        # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€
        Z_safe = torch.where(z_mask, Z, 1e-6)

        # 3D -> 2D í”„ë¡œì ì…˜ (Intrinsics ì ìš©)
        u = self.fx_rgb_tensor * X / Z_safe + self.cx_rgb_tensor
        v = self.fy_rgb_tensor * Y / Z_safe + self.cy_rgb_tensor

        # F.grid_sampleì„ ìœ„í•œ ì •ê·œí™” (0 ~ W-1) -> (-1 ~ 1)
        norm_u = (u / (self.rgb_width - 1.0)) * 2.0 - 1.0
        norm_v = (v / (self.rgb_height - 1.0)) * 2.0 - 1.0

        # (H, W, 2) ìŠ¤íƒ
        normalized_grid = torch.stack([norm_u, norm_v], dim=-1)

        # ìœ íš¨ ìƒ˜í”Œë§ ë§ˆìŠ¤í¬ (ì¹´ë©”ë¼ ì• + ì´ë¯¸ì§€ í”„ë ˆì„ ë‚´ë¶€)
        sampling_mask = z_mask & \
                        (norm_u >= -1.0) & (norm_u <= 1.0) & \
                        (norm_v >= -1.0) & (norm_v <= 1.0)

        return normalized_grid, sampling_mask
    # --- â¬†ï¸ (í•µì‹¬) ìƒˆë¡œìš´ ì •ë ¬ í•¨ìˆ˜ â¬†ï¸ ---



    def rgbd_callback(self, depth_msg, rgb_msg):
        """Depth, RGB ë™ì‹œ ìˆ˜ì‹  ë° ì „ì²´ GPU íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬"""
        t_start = time.perf_counter()

        try:
            # --- 1. ì‹œë§¨í‹± ì˜ˆì¸¡ (CPU/GPU) ---
            # RGB ë©”ì‹œì§€ ë³€í™˜ (BGR8, 640x480)
            rgb_image = self.bridge.imgmsg_to_cv2(rgb_msg, desired_encoding='bgr8')

            if self.config.use_semantic:
                t_sem_start = time.perf_counter()
                # ì‹œë§¨í‹± ì˜ˆì¸¡ (ê²°ê³¼: H_rgb x W_rgb NumPy)
                semantic_mask_rgb_res = self.semantic_model.predict(rgb_image)
                self.timings['semantic'].append((time.perf_counter() - t_sem_start) * 1000)

                if semantic_mask_rgb_res is None:
                    self.get_logger().warn('ì‹œë§¨í‹± ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨', throttle_duration_sec=1.0)
                    return
                
                # GPU ì—…ë¡œë“œ
                mask_tensor_rgb_res = torch.from_numpy(semantic_mask_rgb_res).to(self.device)

            else:
                # ì‹œë§¨í‹± ë¹„í™œì„±í™” ì‹œ
                mask_tensor_rgb_res = None # ë§ˆìŠ¤í¬ í…ì„œ ì—†ìŒ
                self.timings['semantic'].append(0.0) # ì‹œë§¨í‹± ì‹œê°„ 0

            # --- 2. ë°ì´í„° GPU ì—…ë¡œë“œ ---
            depth_image = self.bridge.imgmsg_to_cv2(
                depth_msg, desired_encoding=depth_msg.encoding
            ).astype(np.float32)
            
            depth_tensor = torch.from_numpy(depth_image).to(self.device) / 1000.0 # mm -> m
            
            # RGBëŠ” í•­ìƒ ì—…ë¡œë“œ
            rgb_tensor_rgb_res = torch.from_numpy(rgb_image).to(self.device)

            # --- 3. 3D ì¬êµ¬ì„± (Depth Frame) ---
            t_depth_start = time.perf_counter()
            pointcloud_cam_depth_frame = self.depth_to_pointcloud_gpu(depth_tensor)
            self.timings['depth_to_pc'].append((time.perf_counter() - t_depth_start) * 1000)

            # --- 4. GPU ê¸°ë°˜ ì •ë ¬ (Projective Alignment) ---
            t_align_start = time.perf_counter()

            # 4.1. 3D í¬ì¸íŠ¸ ë³€í™˜ (Depth Frame -> Color Frame)
            pointcloud_cam_color_frame = self.apply_transform_gpu(
                pointcloud_cam_depth_frame, self.T_color_from_depth_gpu
            )

            # 4.2. 3D -> 2D í”„ë¡œì ì…˜ (Color Frame -> RGB Image)
            normalized_uv_grid, sampling_mask = self.project_points_to_rgb_grid(
                pointcloud_cam_color_frame
            )
            
            # 4.3. grid_sampleì„ ìœ„í•œ í…ì„œ ì¤€ë¹„
            # (1, 3, H_r, W_r) - Bilinear
            rgb_for_interp = rgb_tensor_rgb_res.permute(2, 0, 1).float().unsqueeze(0)
            # (1, H_d, W_d, 2)
            normalized_uv_for_grid_sample = normalized_uv_grid.unsqueeze(0)

            # 4.4. GPU ìƒ˜í”Œë§ (RGBëŠ” í•­ìƒ ìˆ˜í–‰)
            rgb_aligned_interp = F.grid_sample(
                rgb_for_interp, normalized_uv_for_grid_sample, 
                mode='bilinear', padding_mode='zeros', align_corners=False
            )
            
            # --- â¬‡ï¸ ìˆ˜ì •ëœ ë¶€ë¶„ (ì¡°ê±´ë¶€ ë§ˆìŠ¤í¬ ìƒì„±) â¬‡ï¸ ---
            if self.config.use_semantic and mask_tensor_rgb_res is not None:
                # (1, 1, H_r, W_r) - Nearest
                mask_for_interp = mask_tensor_rgb_res.float().unsqueeze(0).unsqueeze(0)
                mask_aligned_interp = F.grid_sample(
                    mask_for_interp, normalized_uv_for_grid_sample, 
                    mode='nearest', padding_mode='zeros', align_corners=False
                )
                mask_aligned = mask_aligned_interp.squeeze().long()
            else:
                # ì‹œë§¨í‹±ì´ êº¼ì§„ ê²½ìš°, (H_d, W_d) ëª¨ì–‘ì˜ 0 (ë°°ê²½) í…ì„œ ìƒì„±
                mask_aligned = torch.zeros(
                    self.depth_shape_hw, 
                    device=self.device, 
                    dtype=torch.long
                )
            # --- â¬†ï¸ ìˆ˜ì •ëœ ë¶€ë¶„ â¬†ï¸ ---

            # 4.5. ê²°ê³¼ í…ì„œ (H_d, W_d)
            rgb_aligned_bgr = rgb_aligned_interp.squeeze().permute(1, 2, 0).to(torch.uint8)
            # mask_alignedëŠ” ìœ„ ì¡°ê±´ë¬¸ì—ì„œ í•­ìƒ ì •ì˜ë¨

            # 4.6. ìœ íš¨í•˜ì§€ ì•Šì€ í¬ì¸íŠ¸/ìƒ˜í”Œë§ ë§ˆìŠ¤í‚¹
            invalid_mask = (depth_tensor <= 0.01) | (~sampling_mask)
            
            rgb_aligned_bgr[invalid_mask] = 0
            mask_aligned[invalid_mask] = 0 # 0 = background
            pointcloud_cam_depth_frame[invalid_mask] = 0.0 # Z=0 -> invalid

            self.timings['align_gpu'].append((time.perf_counter() - t_align_start) * 1000)

            # --- 5. TF ì¡°íšŒ (CPU) ë° ì¢Œí‘œ ë³€í™˜ (GPU) ---
            t_tf_start = time.perf_counter()
            transform = self.tf_buffer.lookup_transform(
                self.target_frame, self.source_frame, rclpy.time.Time()
            )
            transform_matrix = self.transform_to_matrix(transform)
            
            transformed_cloud = self.apply_transform_gpu(pointcloud_cam_depth_frame, transform_matrix)
            self.timings['transform'].append((time.perf_counter() - t_tf_start) * 1000)

            # --- 6. ë©”ì‹œì§€ ë°œí–‰ (PCL, BEV) ---
            stamp = depth_msg.header.stamp

            t_pcl_start = time.perf_counter()
            self.process_and_publish_semantic_pointcloud(
                transformed_cloud, rgb_aligned_bgr, mask_aligned, stamp
            )
            self.timings['pcl_pub'].append((time.perf_counter() - t_pcl_start) * 1000)

            t_bev_start = time.perf_counter()
            self.process_and_publish_semantic_bev(
                transformed_cloud, mask_aligned, stamp
            )
            self.timings['bev_pub'].append((time.perf_counter() - t_bev_start) * 1000)

            # --- 7. íƒ€ì´ë° ê¸°ë¡ ---
            self.timings['total'].append((time.perf_counter() - t_start) * 1000)
            self._report_stats()

        except TransformException as e:
            self.get_logger().warn(f'TF ë³€í™˜ ì‹¤íŒ¨: {e}', throttle_duration_sec=1.0)
        except Exception as e:
            self.get_logger().error(f'Semantic PCL/BEV ì²˜ë¦¬ ì˜¤ë¥˜: {e}')
            import traceback
            self.get_logger().error(traceback.format_exc())





    def depth_to_pointcloud_gpu(self, depth_tensor):
        """GPUë¥¼ ì´ìš©í•œ Depth to Point Cloud ë³€í™˜ (ì¹´ë©”ë¼ ì¢Œí‘œê³„)"""
        z = depth_tensor
        x = (self.u_grid_d - self.cx_d_tensor) * z / self.fx_d_tensor
        y = (self.v_grid_d - self.cy_d_tensor) * z / self.fy_d_tensor
        return torch.stack([x, y, z], dim=-1) # (H, W, 3)

    def apply_transform_gpu(self, points, matrix):
        """GPUë¥¼ ì´ìš©í•œ ì¢Œí‘œ ë³€í™˜"""
        original_shape = points.shape
        points_flat = points.reshape(-1, 3)
        
        # ë§¤íŠ¸ë¦­ìŠ¤ê°€ NumPyë¼ë©´ GPU í…ì„œë¡œ ë³€í™˜
        if isinstance(matrix, np.ndarray):
            matrix_tensor = torch.from_numpy(matrix).to(self.device, dtype=torch.float32)
        else:
            matrix_tensor = matrix # ì´ë¯¸ GPU í…ì„œ (e.g., T_color_from_depth_gpu)

        ones = torch.ones((points_flat.shape[0], 1), device=self.device, dtype=torch.float32)
        homogeneous = torch.cat([points_flat, ones], dim=1)
        transformed = torch.mm(homogeneous, matrix_tensor.T)
        return transformed[:, :3].reshape(original_shape)

    def transform_to_matrix(self, transform):
        """ROS Transform ë©”ì‹œì§€ë¥¼ 4x4 ë™ì°¨ ë³€í™˜ í–‰ë ¬(NumPy)ë¡œ ë³€í™˜"""
        t = transform.transform.translation
        translation = np.array([t.x, t.y, t.z])
        r = transform.transform.rotation
        quat = [r.w, r.x, r.y, r.z] # transforms3d (w, x, y, z) ìˆœì„œ
        rotation_matrix = quat2mat(quat)
        matrix = np.eye(4)
        matrix[:3, :3] = rotation_matrix
        matrix[:3, 3] = translation
        return matrix

    def process_and_publish_semantic_pointcloud(
        self, transformed_cloud, rgb_aligned_bgr, mask_aligned, stamp
    ):
        """Semantic 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œë¥¼ ë‹¤ìš´ìƒ˜í”Œë§, íŒ¨í‚¹ í›„ ë°œí–‰"""

        # 1. ë‹¤ìš´ìƒ˜í”Œë§ (GPU)
        points = transformed_cloud[::self.downsample_y, ::self.downsample_x, :]
        colors_bgr = rgb_aligned_bgr[::self.downsample_y, ::self.downsample_x, :]
        labels = mask_aligned[::self.downsample_y, ::self.downsample_x]

        # 2. Flatten (GPU)
        points_flat = points.reshape(-1, 3)
        colors_flat_bgr = colors_bgr.reshape(-1, 3)
        labels_flat = labels.reshape(-1)

        # 3. ìœ íš¨í•œ í¬ì¸íŠ¸ í•„í„°ë§ (Z > 0)
        # (invalid_maskì—ì„œ Z=0ìœ¼ë¡œ ì„¤ì •í–ˆìœ¼ë¯€ë¡œ, 0.01ë³´ë‹¤ í° ê²ƒë§Œ ìœ íš¨)
        valid_mask = points_flat[:, 2] > 0.01 
        
        points_valid = points_flat[valid_mask]
        colors_valid_bgr = colors_flat_bgr[valid_mask]
        labels_valid = labels_flat[valid_mask]

        num_points = points_valid.shape[0]
        if num_points == 0:
            return

        # 4. RGB íŒ¨í‚¹ (GPU)
        r = colors_valid_bgr[:, 2].long()
        g = colors_valid_bgr[:, 1].long()
        b = colors_valid_bgr[:, 0].long()
        
        rgb_packed_gpu = (r << 16) | (g << 8) | b
        rgb_float32_gpu = rgb_packed_gpu.to(torch.uint32).view(torch.float32)

        # 5. Label íŒ¨í‚¹ (GPU)
        labels_float32_gpu = labels_valid.long().to(torch.uint32).view(torch.float32)

        # 6. (X, Y, Z, RGB, Label) ë°ì´í„° ê²°í•© (GPU)
        data_gpu = torch.stack(
            [
                points_valid[:, 0], 
                points_valid[:, 1], 
                points_valid[:, 2], 
                rgb_float32_gpu, 
                labels_float32_gpu
            ],
            dim=-1 # (N, 5)
        )

        # 7. GPU -> CPU ì „ì†¡
        data_np = data_gpu.cpu().numpy()

        # 8. PointCloud2 ë©”ì‹œì§€ ìƒì„± (CPU)
        pointcloud_msg = self._create_semantic_cloud_from_data(
            data_np, stamp, self.target_frame
        )

        # 9. ë°œí–‰
        self.sem_pc_pub.publish(pointcloud_msg)
    
    def process_and_publish_semantic_bev(
        self, transformed_cloud, mask_aligned, stamp
    ):
        """
        'transformed_cloud' (H, W, 3)ì™€ 'mask_aligned' (H, W) GPU í…ì„œë¥¼ ì‚¬ìš©í•˜ì—¬
        GPUì—ì„œ Semantic BEV ë§µì„ ìƒì„±í•˜ê³  ë°œí–‰í•©ë‹ˆë‹¤. (Label í•„ë“œ í¬í•¨)
        """

        # 1. Flatten (GPU)
        x_flat = transformed_cloud[..., 0].ravel()
        y_flat = transformed_cloud[..., 1].ravel()
        z_flat = transformed_cloud[..., 2].ravel()
        labels_flat = mask_aligned.ravel().long()

        # 2. Z-í•„í„° ë§ˆìŠ¤í¬ (GPU)
        mask = (z_flat > self.z_min_t) & (z_flat < self.z_max_t)

        # --- â¬‡ï¸ ìˆ˜ì •ëœ ë¶€ë¶„ (ì¡°ê±´ë¶€ ì‹œë§¨í‹± í•„í„°ë§) â¬‡ï¸ ---
        # 3. ì‹œë§¨í‹± í•„í„° ë§ˆìŠ¤í¬ (GPU)
        if self.config.use_semantic:
            ignore_mask = torch.zeros_like(labels_flat, dtype=torch.bool)
            for label in self.bev_ignore_labels: 
                ignore_mask |= (labels_flat == label)
            
            mask &= ~ignore_mask
        # --- â¬†ï¸ ìˆ˜ì •ëœ ë¶€ë¶„ â¬†ï¸ ---

        # 4. ì›”ë“œ ì¢Œí‘œ -> ê·¸ë¦¬ë“œ ì¸ë±ìŠ¤ ë³€í™˜ (GPU)
        grid_c = ((x_flat - self.grid_origin_x_t) / self.resolution_t).long()
        grid_r = ((y_flat - self.grid_origin_y_t) / self.resolution_t).long()

        # 5. ë°”ìš´ë”ë¦¬ ì²´í¬ ë§ˆìŠ¤í¬ (GPU)
        mask &= (grid_c >= 0) & (grid_c < self.cells_x) & \
                (grid_r >= 0) & (grid_r < self.cells_y)

        # 6. ìœ íš¨í•œ í¬ì¸íŠ¸ë§Œ í•„í„°ë§ (GPU)
        valid_z = z_flat[mask]
        if valid_z.shape[0] == 0:
            return

        valid_labels = labels_flat[mask]
        valid_r = grid_r[mask]
        valid_c = grid_c[mask]

        # 7. 2D ì¸ë±ìŠ¤ -> 1D ì„ í˜• ì¸ë±ìŠ¤ (GPU)
        linear_indices = valid_r * self.cells_x + valid_c

        # 8. ë°ì´í„° íŒ¨í‚¹ (GPU)
        z_shifted = (valid_z * 1000.0).long() << 16
        packed_data = z_shifted | valid_labels 
        
        # 9. "Highest Point Wins" (GPU Scatter-Max)
        self.bev_packed_flat.fill_(0)
        self.bev_packed_flat.index_reduce_(
            dim=0,
            index=linear_indices,
            source=packed_data,
            reduce="amax",
            include_self=False
        )

        # 10. ìœ íš¨í•œ ì…€ë§Œ ì¶”ì¶œ (GPU)
        valid_bev_mask = self.bev_packed_flat > 0
        valid_indices_flat = torch.where(valid_bev_mask)[0]
        if valid_indices_flat.shape[0] == 0:
            return
        packed_values = self.bev_packed_flat[valid_bev_mask]

        # 11. ë°ì´í„° ì–¸íŒ¨í‚¹ (GPU)
        height_values_mm = packed_values >> 16
        label_values = (packed_values & 0xFFFF).long() 
        height_values = height_values_mm.float() / 1000.0

        # 12. 1D ì¸ë±ìŠ¤ -> 2D ì¸ë±ìŠ¤ -> ì›”ë“œ ì¢Œí‘œ (GPU)
        r_idx_bev = torch.div(valid_indices_flat, self.cells_x, rounding_mode='floor')
        c_idx_bev = valid_indices_flat % self.cells_x

        x_world = self.grid_origin_x_t + (c_idx_bev.float() + 0.5) * self.resolution_t
        y_world = self.grid_origin_y_t + (r_idx_bev.float() + 0.5) * self.resolution_t
        z_world = height_values 

        # 13. ë¼ë²¨ -> RGB ìƒ‰ìƒ ë³€í™˜ (GPU)
        # (use_semantic=Falseì´ë©´ label_valuesê°€ ëª¨ë‘ 0ì´ë¯€ë¡œ, ì»¬ëŸ¬ë§µ 0ë²ˆ(ê²€ì€ìƒ‰)ì´ ì¡°íšŒë¨)
        rgb_float32_gpu = self._label_to_color_gpu(label_values)
        labels_float32_gpu = label_values.long().to(torch.uint32).view(torch.float32)

        # 14. (X, Y, Z, RGB, Label) ë°ì´í„° ê²°í•© (GPU)
        bev_data_gpu = torch.stack(
            [x_world, y_world, z_world, rgb_float32_gpu, labels_float32_gpu],
            dim=-1 # (N, 5)
        )

        # 15. GPU -> CPU ì „ì†¡
        bev_data_np = bev_data_gpu.cpu().numpy()

        # 16. PointCloud2 ë©”ì‹œì§€ ìƒì„± (CPU)
        bev_msg = self._create_semantic_cloud_from_data(
            bev_data_np, stamp, self.target_frame
        )

        # 17. ë°œí–‰
        self.sem_bev_pub.publish(bev_msg)


    def _label_to_color_gpu(self, labels):
        """GPU ì‹œë§¨í‹± ë¼ë²¨ í…ì„œ(long)ë¥¼ íŒ¨í‚¹ëœ float32 RGB í…ì„œë¡œ ë³€í™˜"""
        colors_uint8 = self.semantic_colormap_gpu[labels]

        r = colors_uint8[:, 0].long()
        g = colors_uint8[:, 1].long()
        b = colors_uint8[:, 2].long()

        rgb_packed_gpu = (r << 16) | (g << 8) | b
        return rgb_packed_gpu.to(torch.uint32).view(torch.float32)

    def _create_semantic_cloud_from_data(self, data_np, stamp, frame_id):
        """
        (N, 5) [x, y, z, rgb_float32, label_float32] NumPy ë°°ì—´ë¡œ
        Semantic PointCloud2 ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (PCLê³¼ BEV ê³µí†µ)
        """
        header = Header(stamp=stamp, frame_id=frame_id)
        num_points = data_np.shape[0]
        
        # PCL/BEV ëª¨ë‘ 5ê°œ í•„ë“œ (x,y,z,rgb,label) ì‚¬ìš©
        fields = self.semantic_pointcloud_fields
        point_step = self.point_step_pcl

        return PointCloud2(
            header=header,
            height=1,
            width=num_points,
            fields=fields,
            is_bigendian=False,
            point_step=point_step,
            row_step=point_step * num_points,
            data=data_np.astype(np.float32).tobytes(),
            is_dense=True,
        )

    def _report_stats(self):
        """ì„±ëŠ¥ í†µê³„ ì¶œë ¥"""
        if time.time() - self.last_report_time < 2.0: # 2ì´ˆë§ˆë‹¤
            return
            
        if not self.timings['total']:
            return

        avg_total = np.mean(self.timings['total'])
        fps = 1000.0 / avg_total
        avg_sem = np.mean(self.timings['semantic'])
        avg_align = np.mean(self.timings['align_gpu'])
        avg_depth = np.mean(self.timings['depth_to_pc'])
        avg_tf = np.mean(self.timings['transform'])
        avg_pcl = np.mean(self.timings['pcl_pub'])
        avg_bev = np.mean(self.timings['bev_pub'])

        msg = f"\nğŸ“Š [SemanticPCL-BEV] FPS: {fps:.1f} Hz (Total: {avg_total:.1f} ms)\n" \
              f"  â”œâ”€ Semantic : {avg_sem:6.1f} ms\n" \
              f"  â”œâ”€ Depthâ†’PC : {avg_depth:6.1f} ms\n" \
              f"  â”œâ”€ Align GPU: {avg_align:6.1f} ms (Projective)\n" \
              f"  â”œâ”€ Transform: {avg_tf:6.1f} ms\n" \
              f"  â”œâ”€ PCL Pub  : {avg_pcl:6.1f} ms\n" \
              f"  â””â”€ BEV Pub  : {avg_bev:6.1f} ms"
        
        self.get_logger().info(msg)
        self.last_report_time = time.time()


def main(args=None):
    """ë©”ì¸ í•¨ìˆ˜"""
    rclpy.init(args=args)
    
    node = SemanticPointCloudBEVNode()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.get_logger().info('Shutting down...')
        node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
