#!/usr/bin/env python3
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms
from torch.cuda.amp import autocast # Half Precisionì„ ìœ„í•´ import
import numpy as np
import cv2
from PIL import Image as PILImage
import time
import argparse
from collections import deque
from dataclasses import dataclass, field
from typing import Optional, Dict
from tqdm import tqdm
import os
import logging

# transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸
from transformers import (
    SegformerForSemanticSegmentation,
    AutoImageProcessor,
    Mask2FormerForUniversalSegmentation,
    SegformerConfig
)

# ë¡œê±° ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ============================================================================
# 1. ì˜ì¡´ì„± ì½”ë“œ (Config)
# (ì´ì „ê³¼ ë™ì¼)
# ============================================================================
OBJECT_CLASSES = {
    'background': 0, 'barricade': 1, 'bench': 2, 'bicycle': 3,
    'bollard': 4, 'bus': 5, 'car': 6, 'carrier': 7, 'cat': 8,
    'chair': 9, 'dog': 10, 'fire_hydrant': 11, 'kiosk': 12,
    'motorcycle': 13, 'movable_signage': 14, 'parking_meter': 15,
    'person': 16, 'pole': 17, 'potted_plant': 18,
    'power_controller': 19, 'scooter': 20, 'stop': 21,
    'stroller': 22, 'table': 23, 'traffic_light': 24,
    'traffic_light_controller': 25, 'traffic_sign': 26,
    'tree_trunk': 27, 'truck': 28, 'wheelchair': 29
}
SURFACE_CLASSES = {
    'background': 0, 'caution_zone': 1, 'bike_lane': 2, 'alley': 3,
    'roadway': 4, 'braille_block': 5, 'sidewalk': 6
}
CITYSCAPES_CLASSES = {
    "road": 0, "sidewalk": 1, "building": 2, "wall": 3,
    "fence": 4, "pole": 5, "traffic light": 6, "traffic sign": 7,
    "vegetation": 8, "terrain": 9, "sky": 10, "person": 11,
    "rider": 12, "car": 13, "truck": 14, "bus": 15, "train": 16,
    "motorcycle": 17, "bicycle": 18
}

@dataclass
class CameraIntrinsics:
    """Camera intrinsic parameters (ì´ ìŠ¤í¬ë¦½íŠ¸ì—ì„œëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)"""
    fx: float
    fy: float
    cx: float
    cy: float

@dataclass
class ReconstructionConfig:
    """Main configuration for segmentation"""
    use_semantic: bool = True
    model_type: str ="maskformer-cityscapes"
    
    # --- ì¤‘ìš” ---
    # ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ì˜ ê²½ë¡œë¥¼ ì‹¤ì œ íŒŒì¼ ìœ„ì¹˜ë¡œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    custom_object_model_path: str = "models/dynamic_object/best_model2.pth"
    custom_surface_model_path: str = "models/surface/surface_mask_best_lrup.pt"

    segformer_checkpoint: str = "nvidia/segformer-b0-finetuned-cityscapes-1024-1024"
    maskformer_checkpoint: str = "facebook/mask2former-swin-tiny-cityscapes-semantic"
    
    active_model_name: str = field(init=False)
    inference_size: int = field(init=False)
    custom_class_names: Dict[str, int] = field(init=False)
    
    def __post_init__(self):
        # (ì°¸ê³ ) custom-objectì˜ inference_sizeëŠ” ë” ì´ìƒ ì „ì²˜ë¦¬ì— ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        if self.model_type == "custom-object":
            self.active_model_name = self.custom_object_model_path
            self.custom_class_names = OBJECT_CLASSES.copy()
            self.inference_size = 512 
        elif self.model_type == "custom-surface":
            self.active_model_name = self.custom_surface_model_path
            self.custom_class_names = SURFACE_CLASSES.copy()
            self.inference_size = 512
        elif self.model_type == "segformer-cityscapes":
            self.active_model_name = self.segformer_checkpoint
            self.custom_class_names = CITYSCAPES_CLASSES.copy()
            self.inference_size = 512
        elif self.model_type == "maskformer-cityscapes":
            self.active_model_name = self.maskformer_checkpoint
            self.custom_class_names = CITYSCAPES_CLASSES.copy()
            self.inference_size = 384
        else:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” model_typeì…ë‹ˆë‹¤: {self.model_type}")

    @property
    def num_custom_classes(self) -> int:
        return len(self.custom_class_names)
    
    @property
    def idx_to_class(self) -> Dict[int, str]:
        return {v: k for k, v in self.custom_class_names.items()}

# ============================================================================
# 2. ì˜ì¡´ì„± ì½”ë“œ (Model)
# ============================================================================

# --- â¬‡ï¸ ìˆ˜ì •ëœ ë¶€ë¶„ 1: CustomSegFormer í´ë˜ìŠ¤ ìˆ˜ì • â¬‡ï¸ ---
class CustomSegFormer(nn.Module):
    """Custom trained SegFormer model (ROS ì½”ë“œì˜ DirectSegFormerì™€ ë™ì¼ êµ¬ì¡°)"""
    def __init__(self, num_classes: int = 30, pretrained_name: str = "nvidia/mit-b0"):
        super().__init__()
        try:
            # ì†ì„± ì´ë¦„ì„ 'original_model'ë¡œ ë³€ê²½
            self.original_model = SegformerForSemanticSegmentation.from_pretrained(
                pretrained_name,
                num_labels=num_classes, 
                ignore_mismatched_sizes=True,
                trust_remote_code=True,
                torch_dtype=torch.float32,
                use_safetensors=True,
            )
        except (ValueError, OSError) as e:
            if "torch.load" in str(e) or "is not a local folder and is not a valid model identifier" in str(e):
                logger.warning(f"Warning: {e}")
                logger.warning("Creating model architecture without pretrained weights...")
                config = SegformerConfig.from_pretrained(pretrained_name)
                config.num_labels = num_classes
                # ì†ì„± ì´ë¦„ì„ 'original_model'ë¡œ ë³€ê²½
                self.original_model = SegformerForSemanticSegmentation(config)
            else:
                raise e
                
    def forward(self, x):
        # 'original_model'ì„ ì‚¬ìš©í•˜ì—¬ forward
        outputs = self.original_model(pixel_values=x)
        return outputs.logits
# --- â¬†ï¸ ìˆ˜ì •ëœ ë¶€ë¶„ 1 â¬†ï¸ ---


class SemanticModel:
    """Unified interface for different semantic segmentation models"""
    def __init__(self, config, device, logger_instance=None):
        self.config = config
        self.device = device
        self.logger = logger_instance
        self.model = None
        self.image_processor = None
        self.enable_half = (self.device.type == 'cuda') # Half Precision ì‚¬ìš© ì—¬ë¶€
        self.inference_size_hw = (config.inference_size, config.inference_size)
        self.inference_size_wh = (config.inference_size, config.inference_size)
        
        if not config.use_semantic:
            self._log("Semantic segmentation disabled - using RGB only")
            return
            
        self.model_type = config.model_type
        self._load_model()
        
        if self.enable_half:
            self._log("âš¡ Half Precision (FP16) enabled for inference")

    def _log(self, msg):
        if self.logger:
            self.logger.info(msg)
        else:
            print(msg)

    def _load_model(self):
        """Load the specified model"""
        if self.model_type == "custom-object":
            self._load_custom_model()
        elif self.model_type == "custom-surface":
            # (ì°¸ê³ ) custom-surfaceê°€ custom-objectì™€ ë™ì¼í•œ êµ¬ì¡°/ê°€ì¤‘ì¹˜ ë¡œë”©ì„ ì“´ë‹¤ë©´
            # ì—¬ê¸°ë„ ë™ì¼í•˜ê²Œ ìˆ˜ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì¼ë‹¨ custom-objectë§Œ ìˆ˜ì •í•©ë‹ˆë‹¤.
            self._load_custom_model() 
        elif self.model_type == "segformer-cityscapes":
            self._load_segformer()
        elif self.model_type == "maskformer-cityscapes":
            self._load_maskformer()
        else:
            raise ValueError(f"Unknown model type: {self.model_type}")

    # --- â¬‡ï¸ ìˆ˜ì •ëœ ë¶€ë¶„ 2: _load_custom_model í‚¤ ë§¤í•‘ ìˆ˜ì • â¬‡ï¸ ---
    def _load_custom_model(self):
        """Load custom trained SegFormer (ROS-compatible)"""
        # 1. CustomSegFormer (ë‚´ë¶€ì— original_model ë³´ìœ ) ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.model = CustomSegFormer(num_classes=self.config.num_custom_classes)
        
        model_path = self.config.active_model_name

        if not os.path.exists(model_path):
            self._log(f"âš ï¸ ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
            self._log("ê²½ê³ : ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—†ì´ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            self.model.to(self.device)
            self.model.eval()
            self.transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ])
            return

        try:
            # 2. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = torch.load(
                model_path,
                map_location=self.device,
                weights_only=False
            )
            
            # 'model' í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸ (í•™ìŠµ ì‹œ ì €ì¥ ë°©ì‹ì— ë”°ë¼)
            if 'model' in checkpoint:
                checkpoint = checkpoint['model']
            
            # 3. state_dict í‚¤ ì •ë¦¬ (ROS ì½”ë“œì™€ ë™ì¼í•˜ê²Œ)
            new_state_dict = {}
            for key, value in checkpoint.items():
                if key.startswith('segformer.') or key.startswith('decode_head.'):
                    # í‚¤ ì ‘ë‘ì‚¬ë¥¼ 'original_model.'ë¡œ ë³€ê²½
                    new_key = 'original_model.' + key
                else:
                    # 'original_model.'ì´ ì´ë¯¸ ë¶™ì–´ìˆê±°ë‚˜ ë‹¤ë¥¸ í‚¤ì¼ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    new_key = key
            
                new_state_dict[new_key] = value

            # 4. self.model (CustomSegFormer)ì— ê°€ì¤‘ì¹˜ ë¡œë“œ
            self.model.load_state_dict(new_state_dict, strict=False)

            self._log(f"âœ… Custom model loaded from {model_path} (ROS-compatible keys)")

        except Exception as e:
            self._log(f"âš ï¸ Model loading failed: {e}")
            self._log("Using model without pretrained weights")
            
        self.model.to(self.device)
        self.model.eval()
        # 5. ì „ì²˜ë¦¬ ì •ì˜ (ë¦¬ì‚¬ì´ì¦ˆ ì—†ìŒ)
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])
    # --- â¬†ï¸ ìˆ˜ì •ëœ ë¶€ë¶„ 2 â¬†ï¸ ---

    def _load_segformer(self):
        """Load SegFormer model"""
        model_name = self.config.active_model_name
        self.image_processor = AutoImageProcessor.from_pretrained(model_name)
        self.model = SegformerForSemanticSegmentation.from_pretrained(
            model_name,
            ignore_mismatched_sizes=True,
            trust_remote_code=True,
            torch_dtype=torch.float32,
            use_safetensors=True,
        )
        self.model.to(self.device)
        self.model.eval()
        self._log("âœ… SegFormer model loaded")

    def _load_maskformer(self):
        """Load MaskFormer model"""
        model_name = self.config.active_model_name
        self.image_processor = AutoImageProcessor.from_pretrained(model_name)
        self.model = Mask2FormerForUniversalSegmentation.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
        self._log("âœ… MaskFormer-COCO model loaded")

    def predict(self, rgb_image):
        """Run semantic segmentation on RGB image (BGR OpenCV format)"""
        if not self.config.use_semantic:
            return None
        
        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ë¶„ê¸°
        if self.model_type == "custom-object":
            return self._predict_custom(rgb_image)
        elif self.model_type == "custom-surface":
            return self._predict_custom(rgb_image) # custom-surfaceë„ ë™ì¼ ë¡œì§ ê°€ì •
        elif self.model_type == "segformer-cityscapes":
            return self._predict_segformer(rgb_image)
        elif self.model_type == "maskformer-cityscapes":
            return self._predict_maskformer(rgb_image)

    # --- â¬‡ï¸ ìˆ˜ì •ëœ ë¶€ë¶„ 3: _predict_custom ë¦¬ì‚¬ì´ì¦ˆ ì œê±° â¬‡ï¸ ---
    def _predict_custom(self, rgb_image):
        h_orig, w_orig = rgb_image.shape[:2]
        
        # 1. (ì œê±°) ì‚¬ì „ ë¦¬ì‚¬ì´ì¦ˆ ë¡œì§ ì‚­ì œ
        # 2. ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ë°”ë¡œ BGR -> RGB ë³€í™˜
        rgb_image_rgb = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)
        pil_image = PILImage.fromarray(rgb_image_rgb)
        
        # 3. ì „ì²˜ë¦¬ ì ìš© (ë¦¬ì‚¬ì´ì¦ˆ ì—†ìŒ)
        input_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            # 4. Half Precision (autocast) ì ìš©
            with autocast(enabled=self.enable_half):
                logits = self.model(input_tensor) # self.modelì€ CustomSegFormer
            
            # FP16 ì¶œë ¥ -> FP32ë¡œ ë³€í™˜ (F.interpolate í˜¸í™˜ì„±)
            if self.enable_half:
                logits = logits.float()
                
        # 5. ì›ë³¸ í•´ìƒë„ë¡œ ì—…ìƒ˜í”Œë§ (ROS ì½”ë“œì™€ ë™ì¼)
        logits = F.interpolate(
            logits, size=(h_orig, w_orig), mode='bilinear', align_corners=False
        )
        
        pred_mask = torch.argmax(logits, dim=1).squeeze()
        return pred_mask.cpu().numpy().astype(np.uint8)
    # --- â¬†ï¸ ìˆ˜ì •ëœ ë¶€ë¶„ 3 â¬†ï¸ ---

    def _predict_segformer(self, rgb_image):
        h_orig, w_orig = rgb_image.shape[:2]
        
        # (ì°¸ê³ ) Segformer/MaskformerëŠ” inference_sizeë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ ,
        # AutoImageProcessorê°€ ë‚´ë¶€ì ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        rgb_image_rgb = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)
        pil_image = PILImage.fromarray(rgb_image_rgb)
        
        inputs = self.image_processor(images=pil_image, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            # Half Precision (autocast) ì ìš©
            with autocast(enabled=self.enable_half):
                outputs = self.model(**inputs)
        
        if self.enable_half:
            outputs.logits = outputs.logits.float()
            
        result = self.image_processor.post_process_semantic_segmentation(
            outputs, target_sizes=[(h_orig, w_orig)]
        )[0]
        
        return result.cpu().numpy().astype(np.uint8)

    def _predict_maskformer(self, rgb_image):
        h_orig, w_orig = rgb_image.shape[:2]

        rgb_image_rgb = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)
        pil_image = PILImage.fromarray(rgb_image_rgb)

        inputs = self.image_processor(images=pil_image, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            # Half Precision (autocast) ì ìš©
            with autocast(enabled=self.enable_half):
                outputs = self.model(**inputs)
                
        if self.enable_half:
            if outputs.class_queries_logits is not None:
                outputs.class_queries_logits = outputs.class_queries_logits.float()
            if outputs.masks_queries_logits is not None:
                outputs.masks_queries_logits = outputs.masks_queries_logits.float()
                
        result = self.image_processor.post_process_semantic_segmentation(
            outputs, target_sizes=[(h_orig, w_orig)]
        )[0]
        
        return result.cpu().numpy().astype(np.uint8)

# ============================================================================
# 3. ì‹œë§¨í‹± ë§ˆìŠ¤í¬ ì‹œê°í™” í´ë˜ìŠ¤
# (ì´ì „ê³¼ ë™ì¼)
# ============================================================================

class SemanticVisualizer:
    """ì‹œë§¨í‹± ë§ˆìŠ¤í¬ë¥¼ GPUì—ì„œ RGB ì»¬ëŸ¬ë¡œ ë³€í™˜í•˜ê³  ì›ë³¸ ì´ë¯¸ì§€ì™€ ë¸”ë Œë”©"""
    
    def __init__(self, config, device):
        self.config = config
        self.device = device
        self.num_classes = config.num_custom_classes
        self._init_semantic_colormap()
        logger.info(f'GPU ì‹œë§¨í‹± ì»¬ëŸ¬ë§µ ìƒì„± ì™„ë£Œ ({self.num_classes} classes)')

    def _init_semantic_colormap(self):
        """ì‹œë§¨í‹± ë¼ë²¨ì„ RGBë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ GPU ì»¬ëŸ¬ë§µ ìƒì„±"""
        # Cityscapes (19 classes) ì˜ˆì‹œ ì»¬ëŸ¬ë§µ (R, G, B)
        cityscapes_palette = [
            [128, 64, 128], [244, 35, 232], [70, 70, 70], [102, 102, 156], [190, 153, 153],
            [153, 153, 153], [250, 170, 30], [220, 220, 0], [107, 142, 35], [152, 251, 152],
            [70, 130, 180], [220, 20, 60], [255, 0, 0], [0, 0, 142], [0, 0, 70],
            [0, 60, 100], [0, 80, 100], [0, 0, 230], [119, 11, 32]
        ]
        
        colors = torch.zeros((self.num_classes, 3), dtype=torch.uint8, device=self.device)
        
        # í´ë˜ìŠ¤ ì´ë¦„ê³¼ ì¸ë±ìŠ¤ ë§¤í•‘
        for i, (name, idx) in enumerate(self.config.custom_class_names.items()):
            if idx >= self.num_classes:
                continue 
                
            if i < len(cityscapes_palette):
                colors[idx] = torch.tensor(cityscapes_palette[i], dtype=torch.uint8, device=self.device)
            else:
                r = (i * 50) % 255
                g = (i * 90) % 255
                b = (i * 120) % 255
                colors[idx] = torch.tensor([r, g, b], dtype=torch.uint8, device=self.device)
        
        if 0 < self.num_classes:
            colors[0] = torch.tensor([0, 0, 0], dtype=torch.uint8, device=self.device)
        
        self.semantic_colormap_gpu = colors

    def apply_colormap(self, mask_tensor, original_image_bgr_tensor, alpha=0.6):
        """
        GPUì—ì„œ ë§ˆìŠ¤í¬ì— ì»¬ëŸ¬ë§µì„ ì ìš©í•˜ê³  ì›ë³¸ ì´ë¯¸ì§€(BGR)ì™€ ë¸”ë Œë”©í•©ë‹ˆë‹¤.
        """
        colors_rgb = self.semantic_colormap_gpu[mask_tensor.long()]
        colors_bgr = colors_rgb[..., [2, 1, 0]] 
        
        blended_gpu = (
            original_image_bgr_tensor.float() * (1.0 - alpha) + 
            colors_bgr.float() * alpha
        )
        
        return blended_gpu.to(torch.uint8)

# ============================================================================
# 4. ë¹„ë””ì˜¤ ì²˜ë¦¬ ë©”ì¸ í´ë˜ìŠ¤
# (ì´ì „ê³¼ ë™ì¼)
# ============================================================================

class VideoProcessor:
    """ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ê³ , ëª¨ë¸ ì¶”ë¡ ì„ ì‹¤í–‰í•˜ë©°, ê²°ê³¼ ë¹„ë””ì˜¤ë¥¼ ì €ì¥"""
    
    def __init__(self, config: ReconstructionConfig, input_path: str, output_path: str):
        self.config = config
        self.input_path = input_path
        self.output_path = output_path
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        logger.info(f"Using device: {self.device}")
        
        self.model = SemanticModel(config, self.device, logger_instance=self)
        self.visualizer = SemanticVisualizer(config, self.device)
        self.timings = deque(maxlen=200)

    # SemanticModelì´ ì‚¬ìš©í•  ë¡œê¹… ë©”ì†Œë“œ
    def info(self, msg):
        logger.info(msg)

    def process_video(self):
        """ë¹„ë””ì˜¤ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
        logger.info(f"--- ğŸš€ ëª¨ë¸ [{self.config.model_type}] ì²˜ë¦¬ ì‹œì‘ ---")
        
        cap = cv2.VideoCapture(self.input_path)
        if not cap.isOpened():
            logger.error(f"ì˜¤ë¥˜: ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {self.input_path}")
            return

        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        logger.info(f"ì…ë ¥ ë¹„ë””ì˜¤: {width}x{height} @ {fps:.2f} FPS, ì´ {frame_count} í”„ë ˆì„")
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        writer = cv2.VideoWriter(self.output_path, fourcc, fps, (width, height))
        if not writer.isOpened():
            logger.error(f"ì˜¤ë¥˜: ë¹„ë””ì˜¤ íŒŒì¼ì„ ì“¸ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {self.output_path}")
            cap.release()
            return
            
        logger.info(f"ì¶œë ¥ ë¹„ë””ì˜¤ ì €ì¥ ìœ„ì¹˜: {self.output_path}")

        try:
            pbar = tqdm(total=frame_count, desc=f"Processing {self.config.model_type}")
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                t_start = time.perf_counter()
                
                # 5. ì‹œë§¨í‹± ì˜ˆì¸¡ (NumPy BGR ì…ë ¥ -> NumPy (H, W) ë¼ë²¨ ì¶œë ¥)
                pred_mask_np = self.model.predict(frame)
                
                if pred_mask_np is None:
                    logger.warning("ì‹œë§¨í‹± ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨, í”„ë ˆì„ ê±´ë„ˆëœ€")
                    writer.write(frame) # ì›ë³¸ í”„ë ˆì„ ì €ì¥
                    continue

                # 6. ì‹œê°í™” (GPU ê°€ì†)
                frame_gpu = torch.from_numpy(frame).to(self.device)
                mask_gpu = torch.from_numpy(pred_mask_np).to(self.device)
                
                blended_frame_gpu = self.visualizer.apply_colormap(
                    mask_gpu, frame_gpu, alpha=0.6
                )
                
                blended_frame_np = blended_frame_gpu.cpu().numpy()
                
                writer.write(blended_frame_np)
                
                self.timings.append(time.perf_counter() - t_start)
                pbar.update(1)

            pbar.close()

        except Exception as e:
            logger.error(f"ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
        finally:
            cap.release()
            writer.release()
            logger.info("ë¹„ë””ì˜¤ ìº¡ì²˜ ë° ì“°ê¸° ê°ì²´ í•´ì œ ì™„ë£Œ")

        if self.timings:
            avg_time_ms = np.mean(self.timings) * 1000
            avg_fps = 1000.0 / avg_time_ms
            logger.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {self.config.model_type}")
            logger.info(f"   í‰ê·  ì²˜ë¦¬ ì†ë„: {avg_fps:.2f} FPS ({avg_time_ms:.2f} ms/frame)")

# ============================================================================
# 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# (ì´ì „ê³¼ ë™ì¼)
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="MP4 ë¹„ë””ì˜¤ì— 4ê°€ì§€ ì‹œë§¨í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ì„ ì ìš©í•©ë‹ˆë‹¤."
    )
    parser.add_argument(
        "-i", "--input", 
        type=str, 
        required=True, 
        help="ì…ë ¥ MP4 ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ"
    )
    args = parser.parse_args()

    input_path = args.input
    if not os.path.exists(input_path):
        logger.error(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
        return

    # ì²˜ë¦¬í•  ëª¨ë¸ íƒ€ì… ë¦¬ìŠ¤íŠ¸
    model_types_to_run = [
        # "custom-object",
        # "custom-surface",
        "segformer-cityscapes",
        "maskformer-cityscapes"
    ]
    
    base_name = os.path.basename(input_path)
    name_without_ext = os.path.splitext(base_name)[0]
    output_dir = os.path.dirname(input_path) or "." 

    for model_type in model_types_to_run:
        try:
            config = ReconstructionConfig(model_type=model_type)
            
            output_filename = f"{name_without_ext}_{model_type}_output.mp4"
            output_path = os.path.join(output_dir, output_filename)
            
            processor = VideoProcessor(config, input_path, output_path)
            processor.process_video()
            
            # GPU ìºì‹œ í´ë¦¬ì–´ (ë©”ëª¨ë¦¬ ë¶€ì¡± ë°©ì§€)
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        except Exception as e:
            logger.error(f"--- âŒ ëª¨ë¸ [{model_type}] ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ ---")
            logger.error(e)
            import traceback
            traceback.print_exc()


if __name__ == '__main__':
    main()
