# validation_test.py
"""
ì£¼ìš” ê¸°ëŠ¥

ëª¨ë¸ ë¡œë“œ: ì €ì¥ëœ .pth íŒŒì¼ì—ì„œ í•™ìŠµëœ ì •ì±… ë¡œë“œ
ì‹œê°ì  ê²€ì¦: í™˜ê²½ ë Œë”ë§ìœ¼ë¡œ ì‹¤ì œ ì£¼í–‰ í™•ì¸
ë¹„ë””ì˜¤ ì €ì¥: ì£¼í–‰ ê³¼ì •ì„ ë™ì˜ìƒìœ¼ë¡œ ì €ì¥
í†µê³„ ë¶„ì„: ì—¬ëŸ¬ ì—í”¼ì†Œë“œì˜ ì„±ê³µë¥  ë“± ë¶„ì„

ğŸ¯ ì‚¬ìš©ë²•
bashpython validation_test.py
ğŸ“Š ì¶œë ¥ ê²°ê³¼

ì„¼ì„œ ì‹œê°í™”: RGB, Depth, Semantic ì¹´ë©”ë¼ ì´ë¯¸ì§€
ì£¼í–‰ ë¹„ë””ì˜¤: validation_run.avi íŒŒì¼ë¡œ ì €ì¥
ìƒì„¸ í†µê³„: ì„±ê³µë¥ , ì¶©ëŒë¥ , í‰ê·  ë³´ìƒ ë“±

âš™ï¸ ì„¤ì • ë³€ê²½ í¬ì¸íŠ¸

model_path: ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½
num_episodes: í…ŒìŠ¤íŠ¸í•  ì—í”¼ì†Œë“œ ìˆ˜
max_steps: ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜
use_render=True: ì‹œê°í™” í™œì„±í™”

ì´ ì½”ë“œë¡œ í•™ìŠµëœ ì—ì´ì „íŠ¸ê°€ ì‹¤ì œë¡œ ì–¼ë§ˆë‚˜ ì˜ ì£¼í–‰í•˜ëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥!

"""
import numpy as np
import torch
import cv2
import matplotlib.pyplot as plt
from pathlib import Path
import time

from metaurban import SidewalkStaticMetaUrbanEnv
from metaurban.obs.mix_obs import ThreeSourceMixObservation
from metaurban.component.sensors.depth_camera import DepthCamera
from metaurban.component.sensors.rgb_camera import RGBCamera
from metaurban.component.sensors.semantic_camera import SemanticCamera

# ìµœìƒìœ„ ê²½ë¡œì—ì„œ import
# from scripts.autonomous_driving_ppo_modular import PPOAgent, PPOConfig, ObservationProcessor, RewardCalculator

# ìƒëŒ€ ê²½ë¡œë¡œ import
from scripts.core import PPOAgent, PPOConfig, ObservationProcessor, RewardCalculator

# ============================================================================
# í™˜ê²½ ì„¤ì • (ë™ì¼)
# ============================================================================
SENSOR_SIZE = (256, 160)
BASE_ENV_CFG = dict(
    use_render=True,  # ì‹œê°í™”ë¥¼ ìœ„í•´ Trueë¡œ ë³€ê²½
    map='X', 
    manual_control=False, 
    crswalk_density=1, 
    object_density=0.1, 
    walk_on_all_regions=False,
    drivable_area_extension=55, 
    height_scale=1, 
    horizon=300,
    vehicle_config=dict(enable_reverse=True),
    show_sidewalk=True, 
    show_crosswalk=True,
    random_lane_width=True, 
    random_agent_model=True, 
    random_lane_num=True,
    relax_out_of_road_done=True, 
    max_lateral_dist=5.0,
    agent_observation=ThreeSourceMixObservation,
    image_observation=True,
    sensors={
        "rgb_camera": (RGBCamera, *SENSOR_SIZE),                
        "depth_camera": (DepthCamera, *SENSOR_SIZE),
        "semantic_camera": (SemanticCamera, *SENSOR_SIZE),
    },
    log_level=50,
)

class ValidationTester:
    """í•™ìŠµëœ ëª¨ë¸ ê²€ì¦ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: str, device='cuda'):
        self.device = device if torch.cuda.is_available() else 'cpu'
        print(f"Using device: {self.device}")
        
        # ëª¨ë¸ ë¡œë“œ
        self.agent = PPOAgent(PPOConfig(), self.device)
        # self.agent.load_model(model_path,map_location=torch.device('cpu'))
        self.device = torch.device('cpu')
        
    # def load_model(self, filepath):
        """ëª¨ë¸ ë¡œë“œ"""
        # checkpoint = torch.load(model_path,map_location= self.device)
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)

        self.agent.policy.load_state_dict(checkpoint['policy_state_dict'])
        self.agent.value.load_state_dict(checkpoint['value_state_dict'])
        self.agent.policy_optimizer.load_state_dict(checkpoint['policy_optimizer_state_dict'])
        self.agent.value_optimizer.load_state_dict(checkpoint['value_optimizer_state_dict'])
        self.agent.stats = checkpoint['stats']
        self.agent.policy.eval()
        
        # í™˜ê²½ ì´ˆê¸°í™” (ë Œë”ë§ í™œì„±í™”)
        self.env = SidewalkStaticMetaUrbanEnv(BASE_ENV_CFG)
        
        # ìœ í‹¸ë¦¬í‹°
        self.obs_processor = ObservationProcessor()
        self.reward_calculator = RewardCalculator()
        
    def run_episode(self, max_steps=1000, save_video=False):
        """ë‹¨ì¼ ì—í”¼ì†Œë“œ ì‹¤í–‰"""
        # í™˜ê²½ ë¦¬ì…‹
        obs, _ = self.env.reset()
        nav = self.env.vehicle.navigation.get_navi_info()
        obs["goal_vec"] = np.array(nav[:2], dtype=np.float32)
        state = self.obs_processor.preprocess_observation(obs)
        
        episode_reward = 0
        step = 0
        trajectory = []
        
        # ë¹„ë””ì˜¤ ì €ì¥ ì„¤ì •
        if save_video:
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
            out = cv2.VideoWriter('validation_run.avi', fourcc, 20.0, (800, 600))
        
        print(f"Starting validation - Initial goal distance: {np.linalg.norm(obs['goal_vec']):.2f}")
        
        while step < max_steps:
            # í–‰ë™ ì„ íƒ (íƒí—˜ ì—†ì´)
            with torch.no_grad():
                action, _, _ = self.agent.select_action(state)
            
            # í™˜ê²½ ìŠ¤í…
            next_obs, _, done, truncated, info = self.env.step(action.squeeze().numpy())
            
            # goal_vec ì—…ë°ì´íŠ¸
            nav = self.env.vehicle.navigation.get_navi_info()
            next_obs["goal_vec"] = np.array(nav[:2], dtype=np.float32)
            
            # ë³´ìƒ ê³„ì‚°
            reward = self.reward_calculator.compute_reward(obs, action, next_obs, done, info)
            
            # ìƒíƒœ ì •ë³´ ì €ì¥
            trajectory.append({
                'step': step,
                'action': action.squeeze().numpy(),
                'reward': reward,
                'goal_distance': np.linalg.norm(next_obs["goal_vec"]),
                'speed': info.get('speed', 0),
                'crash': info.get('crash', False),
                'out_of_road': info.get('out_of_road', False),
                'arrive_dest': info.get('arrive_dest', False)
            })
            
            # ë Œë”ë§ ë° ë¹„ë””ì˜¤ ì €ì¥
            if save_video:
                frame = self.env.render(mode='rgb_array')
                if frame is not None:
                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                    frame = cv2.resize(frame, (800, 600))
                    out.write(frame)
            
            episode_reward += reward
            step += 1
            
            # ìƒíƒœ ì¶œë ¥
            if step % 50 == 0:
                goal_dist = np.linalg.norm(next_obs["goal_vec"])
                speed = info.get('speed', 0)
                print(f"Step {step}: Reward={reward:.2f}, Goal_dist={goal_dist:.2f}, Speed={speed:.1f}")
            
            # ì¢…ë£Œ ì¡°ê±´
            if done or truncated:
                if info.get('arrive_dest', False):
                    # print(f"SUCCESS! Arrived at destination in {step} steps!")
                    continue
                elif info.get('crash', False):
                    print(f"CRASH! Episode ended at step {step}")
                elif info.get('out_of_road', False):
                    print(f"OUT OF ROAD! Episode ended at step {step}")
                break
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            obs = next_obs
            state = self.obs_processor.preprocess_observation(obs)
        
        if save_video:
            out.release()
            print("Video saved as 'validation_run.avi'")
        
        print(f"Episode completed - Steps: {step}, Total Reward: {episode_reward:.2f}")
        return trajectory, episode_reward, step
    
    def run_multiple_episodes(self, num_episodes=5):
        """ì—¬ëŸ¬ ì—í”¼ì†Œë“œ ì‹¤í–‰ ë° í†µê³„"""
        results = []
        
        for i in range(num_episodes):
            print(f"\n=== Episode {i+1}/{num_episodes} ===")
            trajectory, reward, steps = self.run_episode()
            
            # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
            success = any(t['arrive_dest'] for t in trajectory)
            crash = any(t['crash'] for t in trajectory)
            
            results.append({
                'episode': i+1,
                'reward': reward,
                'steps': steps,
                'success': success,
                'crash': crash,
                'final_goal_distance': trajectory[-1]['goal_distance'] if trajectory else float('inf')
            })
        
        # í†µê³„ ì¶œë ¥
        self.print_statistics(results)
        return results
    
    def print_statistics(self, results):
        """í†µê³„ ì¶œë ¥"""
        print("\n" + "="*50)
        print("VALIDATION RESULTS")
        print("="*50)
        
        total_episodes = len(results)
        successes = sum(1 for r in results if r['success'])
        crashes = sum(1 for r in results if r['crash'])
        
        avg_reward = np.mean([r['reward'] for r in results])
        avg_steps = np.mean([r['steps'] for r in results])
        avg_goal_distance = np.mean([r['final_goal_distance'] for r in results])
        
        print(f"Total Episodes: {total_episodes}")
        print(f"Success Rate: {successes/total_episodes*100:.1f}% ({successes}/{total_episodes})")
        print(f"Crash Rate: {crashes/total_episodes*100:.1f}% ({crashes}/{total_episodes})")
        print(f"Average Reward: {avg_reward:.2f}")
        print(f"Average Steps: {avg_steps:.1f}")
        print(f"Average Final Goal Distance: {avg_goal_distance:.2f}")
        
        print("\nIndividual Results:")
        for r in results:
            status = "SUCCESS" if r['success'] else ("CRASH" if r['crash'] else "TIMEOUT")
            print(f"Episode {r['episode']}: {status} - Reward: {r['reward']:.2f}, Steps: {r['steps']}, Goal Dist: {r['final_goal_distance']:.2f}")
    
    def visualize_sensors(self):
        """ì„¼ì„œ ë°ì´í„° ì‹œê°í™”"""
        obs, _ = self.env.reset()
        
        # ì„¼ì„œ ë°ì´í„° ì¶”ì¶œ : visualize image observation
        rgb = obs["image"][..., -1]  # RGB ì´ë¯¸ì§€
        depth = obs["depth"][..., -1]  # Depth ì´ë¯¸ì§€  
        depth = np.concatenate([depth, depth, depth], axis=-1) # align channel
        semantic = obs["semantic"][..., -1]  # Semantic ì´ë¯¸ì§€
        
        # ì‹œê°í™”
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        axes[0].imshow(rgb)
        axes[0].set_title('RGB Camera')
        axes[0].axis('off')
        
        axes[1].imshow(depth, cmap='viridis')
        axes[1].set_title('Depth Camera')
        axes[1].axis('off')
        
        axes[2].imshow(semantic)
        axes[2].set_title('Semantic Camera')
        axes[2].axis('off')
        
        plt.tight_layout()
        plt.savefig('sensor_visualization.png')
        plt.show()
        
        print("Sensor visualization saved as 'sensor_visualization.png'")

def main():
    """ë©”ì¸ ê²€ì¦ í•¨ìˆ˜"""
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì • (ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½í•˜ì„¸ìš”)
    model_path = "checkpoints/final_model.pth"  # ë˜ëŠ” "checkpoints/model_episode_xxx.pth"
    # model_path = "final_model.pth"  # ë˜ëŠ” "checkpoints/model_episode_xxx.pth"
    
    if not Path(model_path).exists():
        print(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ íŒŒì¼ë“¤:")
        checkpoint_dir = Path("checkpoints")
        if checkpoint_dir.exists():
            for file in checkpoint_dir.glob("*.pth"):
                print(f"  {file}")
        return
    
    # ê²€ì¦ í…ŒìŠ¤í„° ì´ˆê¸°í™”
    tester = ValidationTester(model_path)
    
    # ì„¼ì„œ ì‹œê°í™”
    print("Visualizing sensors...")
    tester.visualize_sensors()
    
    # ë‹¨ì¼ ì—í”¼ì†Œë“œ ì‹¤í–‰ (ë¹„ë””ì˜¤ ì €ì¥)
    print("\nRunning single episode with video recording...")
    tester.run_episode(save_video=True)
    
    # ì—¬ëŸ¬ ì—í”¼ì†Œë“œ í†µê³„
    print("\nRunning multiple episodes for statistics...")
    tester.run_multiple_episodes(num_episodes=5)

if __name__ == "__main__":
    main()